{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify visible cuda device\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from param import *\n",
    "from lib.model import *\n",
    "from lib.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST\n",
    "Load MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST\n",
    "x_train, y_train, x_test, y_test = load_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load ACGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g = acgan_mnist_model_g_old()\n",
    "g.load_weights(WEIGHT_DIR + \"./acgan_mnist/weight_g_epoch_012.hdf5\")\n",
    "\n",
    "d = acgan_mnist_model_d()\n",
    "d.load_weights(WEIGHT_DIR + \"./acgan_mnist/weight_d_epoch_012.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAGfCAYAAAAKzUbVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9962651390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   [0.39, 0.5, 0.42, 0.33, 0.4, 0.46, 0.44, 0.42, 0.55, 0.51]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   [0.3, 0.51, 0.6, 0.29, 0.5, 0.31, 0.25, 0.22, 0.55, 0.46]\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2]   [0.53, 0.76, 0.48, 0.7, 0.43, 0.73, 0.51, 0.7, 0.67, 0.35]\n",
      "[3, 3, 3, 3, 3, 3, 3, 3, 3, 3]   [0.28, 0.67, 0.71, 0.29, 0.53, 0.3, 0.61, 0.35, 0.34, 0.27]\n",
      "[4, 4, 4, 4, 4, 4, 4, 4, 4, 4]   [0.63, 0.58, 0.61, 0.39, 0.59, 0.44, 0.42, 0.53, 0.36, 0.45]\n",
      "[5, 5, 5, 5, 5, 5, 5, 5, 5, 5]   [0.46, 0.24, 0.37, 0.25, 0.4, 0.42, 0.53, 0.25, 0.81, 0.32]\n",
      "[6, 6, 6, 6, 6, 6, 6, 6, 6, 6]   [0.24, 0.34, 0.56, 0.41, 0.45, 0.45, 0.57, 0.5, 0.29, 0.32]\n",
      "[7, 7, 7, 7, 7, 7, 7, 7, 7, 7]   [0.47, 0.44, 0.82, 0.43, 0.62, 0.58, 0.18, 0.44, 0.79, 0.68]\n",
      "[8, 8, 8, 8, 8, 8, 8, 8, 8, 8]   [0.47, 0.45, 0.3, 0.19, 0.42, 0.35, 0.45, 0.32, 0.43, 0.55]\n",
      "[9, 9, 9, 9, 9, 9, 9, 9, 9, 9]   [0.43, 0.4, 0.28, 0.34, 0.32, 0.3, 0.34, 0.36, 0.31, 0.31]\n"
     ]
    }
   ],
   "source": [
    "def collage(images):\n",
    "    img = (np.concatenate([np.concatenate([s for s in r], axis=1)\n",
    "                           for r in np.split(images, 10)], axis=0) *\n",
    "           127.5 + 127.5).astype(np.uint8)\n",
    "    return np.squeeze(img)\n",
    "\n",
    "noise = np.random.uniform(-1, 1, (100, LATENT_SIZE))\n",
    "sampled_labels = np.array([[i] * 10 for i in range(10)]).reshape(-1, 1)\n",
    "#sampled_target = np.array([[i for i in range(10)] * 10]).reshape(-1, 1)\n",
    "\n",
    "# get a batch to display\n",
    "x_g = g.predict([noise, sampled_labels], verbose=0)\n",
    "\n",
    "img = collage(x_g)\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.savefig('acgan_vis.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "y_pred = []\n",
    "fake = []\n",
    "\n",
    "for i, x in enumerate(x_g):\n",
    "    tmp = d.predict(x.reshape(1, 28, 28, 1))\n",
    "    fake.append(round(float(tmp[0]), 2))\n",
    "    y_pred.append(np.argmax(tmp[1]))\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(\"{}   {}\".format(y_pred, fake))\n",
    "        y_pred = []\n",
    "        fake = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load MNIST CNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.0907250769441\n",
      "Test accuracy: 0.9754\n"
     ]
    }
   ],
   "source": [
    "cnn = Sequential()\n",
    "cnn.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(28, 28, 1)))\n",
    "cnn.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn.add(Dropout(0.25))\n",
    "cnn.add(Flatten())\n",
    "cnn.add(Dense(128, activation='relu'))\n",
    "cnn.add(Dropout(0.5))\n",
    "cnn.add(Dense(10, activation='softmax'))\n",
    "\n",
    "cnn.compile(loss=keras.losses.sparse_categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "cnn.load_weights('test_cnn_mnist.h5')\n",
    "score = cnn.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sample(size):\n",
    "    # Generate samples from g\n",
    "    z = np.random.uniform(-1, 1, (size, LATENT_SIZE))\n",
    "    # Sampled labels\n",
    "    y_sampled = np.random.randint(0, 10, size)\n",
    "    return z, y_sampled\n",
    "\n",
    "def generate_random(g, size):\n",
    "    z, y_sampled = random_sample(size)\n",
    "    x_g = g.predict([z, y_sampled.reshape((-1, 1))], verbose=0)\n",
    "    return x_g, y_sampled\n",
    "\n",
    "def generate_label(g, label, size):\n",
    "    z = np.random.uniform(-1, 1, (size, LATENT_SIZE))\n",
    "    y = np.zeros(size) + label\n",
    "    return g.predict([z, y.reshape((-1, 1))], verbose=0)\n",
    "\n",
    "def collage(images):\n",
    "    img = (np.concatenate([np.concatenate([s for s in r], axis=1)\n",
    "                           for r in np.split(images, 10)], axis=0) *\n",
    "           SCALE + SCALE).astype(np.uint8)\n",
    "    return np.squeeze(img)\n",
    "\n",
    "def show(x):\n",
    "    plt.imshow(x.reshape(28, 28) / 2 + 1, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1, 2, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_r = 1\n",
    "r_g = 0.01\n",
    "dropout = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60600 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60416/60600 [============================>.] - ETA: 0s - loss: 0.3299 - acc: 0.8979Epoch 00001: val_loss improved from inf to 0.06943, saving model to mnist_cnn_r1_g0.01.hdf5\n",
      "60600/60600 [==============================] - 5s 82us/step - loss: 0.3292 - acc: 0.8982 - val_loss: 0.0694 - val_acc: 0.9784\n",
      "Epoch 2/50\n",
      "60416/60600 [============================>.] - ETA: 0s - loss: 0.1041 - acc: 0.9691Epoch 00002: val_loss improved from 0.06943 to 0.05144, saving model to mnist_cnn_r1_g0.01.hdf5\n",
      "60600/60600 [==============================] - 5s 79us/step - loss: 0.1040 - acc: 0.9692 - val_loss: 0.0514 - val_acc: 0.9820\n",
      "Epoch 3/50\n",
      "59904/60600 [============================>.] - ETA: 0s - loss: 0.0768 - acc: 0.9771Epoch 00003: val_loss improved from 0.05144 to 0.04034, saving model to mnist_cnn_r1_g0.01.hdf5\n",
      "60600/60600 [==============================] - 5s 78us/step - loss: 0.0764 - acc: 0.9773 - val_loss: 0.0403 - val_acc: 0.9865\n",
      "Epoch 4/50\n",
      "59904/60600 [============================>.] - ETA: 0s - loss: 0.0605 - acc: 0.9820Epoch 00004: val_loss improved from 0.04034 to 0.03936, saving model to mnist_cnn_r1_g0.01.hdf5\n",
      "60600/60600 [==============================] - 5s 78us/step - loss: 0.0605 - acc: 0.9820 - val_loss: 0.0394 - val_acc: 0.9866\n",
      "Epoch 5/50\n",
      "59904/60600 [============================>.] - ETA: 0s - loss: 0.0519 - acc: 0.9842Epoch 00005: val_loss improved from 0.03936 to 0.03613, saving model to mnist_cnn_r1_g0.01.hdf5\n",
      "60600/60600 [==============================] - 5s 79us/step - loss: 0.0517 - acc: 0.9841 - val_loss: 0.0361 - val_acc: 0.9877\n",
      "Epoch 6/50\n",
      "59904/60600 [============================>.] - ETA: 0s - loss: 0.0456 - acc: 0.9866Epoch 00006: val_loss improved from 0.03613 to 0.03551, saving model to mnist_cnn_r1_g0.01.hdf5\n",
      "60600/60600 [==============================] - 5s 79us/step - loss: 0.0452 - acc: 0.9867 - val_loss: 0.0355 - val_acc: 0.9881\n",
      "Epoch 7/50\n",
      "60544/60600 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9875Epoch 00007: val_loss did not improve\n",
      "60600/60600 [==============================] - 5s 78us/step - loss: 0.0402 - acc: 0.9875 - val_loss: 0.0362 - val_acc: 0.9870\n",
      "Epoch 8/50\n",
      "60032/60600 [============================>.] - ETA: 0s - loss: 0.0346 - acc: 0.9894Epoch 00008: val_loss improved from 0.03551 to 0.03277, saving model to mnist_cnn_r1_g0.01.hdf5\n",
      "60600/60600 [==============================] - 5s 79us/step - loss: 0.0344 - acc: 0.9894 - val_loss: 0.0328 - val_acc: 0.9894\n",
      "Epoch 9/50\n",
      "60032/60600 [============================>.] - ETA: 0s - loss: 0.0333 - acc: 0.9897Epoch 00009: val_loss did not improve\n",
      "60600/60600 [==============================] - 5s 77us/step - loss: 0.0333 - acc: 0.9897 - val_loss: 0.0337 - val_acc: 0.9889\n",
      "Epoch 10/50\n",
      "60416/60600 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9912Epoch 00010: val_loss improved from 0.03277 to 0.03120, saving model to mnist_cnn_r1_g0.01.hdf5\n",
      "60600/60600 [==============================] - 5s 77us/step - loss: 0.0301 - acc: 0.9912 - val_loss: 0.0312 - val_acc: 0.9896\n",
      "Epoch 11/50\n",
      "59904/60600 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9914Epoch 00011: val_loss did not improve\n",
      "60600/60600 [==============================] - 5s 77us/step - loss: 0.0289 - acc: 0.9914 - val_loss: 0.0312 - val_acc: 0.9896\n",
      "Epoch 12/50\n",
      "60416/60600 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9915Epoch 00012: val_loss did not improve\n",
      "60600/60600 [==============================] - 5s 78us/step - loss: 0.0266 - acc: 0.9915 - val_loss: 0.0325 - val_acc: 0.9893\n",
      "Epoch 13/50\n",
      "59904/60600 [============================>.] - ETA: 0s - loss: 0.0259 - acc: 0.9918Epoch 00013: val_loss did not improve\n",
      "60600/60600 [==============================] - 5s 77us/step - loss: 0.0260 - acc: 0.9918 - val_loss: 0.0319 - val_acc: 0.9899\n",
      "Epoch 14/50\n",
      "60032/60600 [============================>.] - ETA: 0s - loss: 0.0240 - acc: 0.9926Epoch 00014: val_loss did not improve\n",
      "60600/60600 [==============================] - 5s 77us/step - loss: 0.0240 - acc: 0.9925 - val_loss: 0.0329 - val_acc: 0.9898\n",
      "Epoch 15/50\n",
      "60032/60600 [============================>.] - ETA: 0s - loss: 0.0221 - acc: 0.9931Epoch 00015: val_loss improved from 0.03120 to 0.03072, saving model to mnist_cnn_r1_g0.01.hdf5\n",
      "60600/60600 [==============================] - 5s 77us/step - loss: 0.0221 - acc: 0.9932 - val_loss: 0.0307 - val_acc: 0.9901\n",
      "Epoch 16/50\n",
      "59904/60600 [============================>.] - ETA: 0s - loss: 0.0230 - acc: 0.9930Epoch 00016: val_loss improved from 0.03072 to 0.02933, saving model to mnist_cnn_r1_g0.01.hdf5\n",
      "60600/60600 [==============================] - 5s 78us/step - loss: 0.0230 - acc: 0.9930 - val_loss: 0.0293 - val_acc: 0.9898\n",
      "Epoch 17/50\n",
      "60032/60600 [============================>.] - ETA: 0s - loss: 0.0210 - acc: 0.9936Epoch 00017: val_loss did not improve\n",
      "60600/60600 [==============================] - 5s 77us/step - loss: 0.0210 - acc: 0.9936 - val_loss: 0.0343 - val_acc: 0.9893\n",
      "Epoch 18/50\n",
      "60032/60600 [============================>.] - ETA: 0s - loss: 0.0213 - acc: 0.9931Epoch 00018: val_loss did not improve\n",
      "60600/60600 [==============================] - 5s 77us/step - loss: 0.0212 - acc: 0.9931 - val_loss: 0.0340 - val_acc: 0.9897\n",
      "Epoch 19/50\n",
      "60032/60600 [============================>.] - ETA: 0s - loss: 0.0201 - acc: 0.9936Epoch 00019: val_loss did not improve\n",
      "60600/60600 [==============================] - 5s 77us/step - loss: 0.0201 - acc: 0.9936 - val_loss: 0.0336 - val_acc: 0.9899\n",
      "Epoch 20/50\n",
      "60544/60600 [============================>.] - ETA: 0s - loss: 0.0209 - acc: 0.9933Epoch 00020: val_loss did not improve\n",
      "60600/60600 [==============================] - 5s 77us/step - loss: 0.0210 - acc: 0.9933 - val_loss: 0.0343 - val_acc: 0.9893\n",
      "Epoch 21/50\n",
      "60416/60600 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.9937Epoch 00021: val_loss did not improve\n",
      "60600/60600 [==============================] - 5s 78us/step - loss: 0.0198 - acc: 0.9937 - val_loss: 0.0351 - val_acc: 0.9897\n",
      "Epoch 00021: early stopping\n",
      "[0.035141535143354302, 0.98970000000000002, 0.020406542863036763, 0.99506666666666665]\n"
     ]
    }
   ],
   "source": [
    "n_train, n_test = len(x_train), len(x_test)\n",
    "\n",
    "ind = np.random.choice(n_train, int(n_train * r_r))\n",
    "x_r, y_r = x_train[ind], y_train[ind]\n",
    "x_g, y_g = generate_random(g, int(n_train * r_g))\n",
    "if r_g == 0:\n",
    "    x_rg, y_rg = x_r, y_r\n",
    "elif r_r == 0:\n",
    "    x_rg, y_rg = x_g, y_g\n",
    "else:\n",
    "    x_rg = np.concatenate((x_r, x_g), axis=0)\n",
    "    y_rg = np.concatenate((y_r, y_g), axis=0)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(28, 28, 1)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "if dropout:\n",
    "    model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "if dropout:\n",
    "    model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.sparse_categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto')\n",
    "weight = 'mnist_cnn_r{}_g{}.hdf5'.format(r_r, r_g)\n",
    "saveBestModel = ModelCheckpoint(weight, monitor='val_loss', verbose=1, \n",
    "                                save_best_only=True, mode='auto')\n",
    "\n",
    "model.fit(x_rg, y_rg,\n",
    "          batch_size=128,\n",
    "          epochs=50,\n",
    "          verbose=1,\n",
    "          callbacks=[earlyStopping, saveBestModel],\n",
    "          validation_data=(x_test, y_test))\n",
    "score_test = model.evaluate(x_test, y_test, verbose=0)\n",
    "score_train = model.evaluate(x_train, y_train, verbose=0)\n",
    "score = [score_test[0], score_test[1], score_train[0], score_train[1]]\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3: Unbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmv_cls = 0\n",
    "keep_ratio = 0\n",
    "replace = True\n",
    "dropout = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "59520/60000 [============================>.] - ETA: 0s - loss: 0.2953 - acc: 0.9100Epoch 00001: val_loss improved from inf to 0.12642, saving model to mnist_cnn_rmvcls0_rmvr0.hdf5\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.2939 - acc: 0.9104 - val_loss: 0.1264 - val_acc: 0.9613\n",
      "Epoch 2/50\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0946 - acc: 0.9722Epoch 00002: val_loss did not improve\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.0945 - acc: 0.9722 - val_loss: 0.1318 - val_acc: 0.9616\n",
      "Epoch 3/50\n",
      "59264/60000 [============================>.] - ETA: 0s - loss: 0.0734 - acc: 0.9779Epoch 00003: val_loss improved from 0.12642 to 0.11073, saving model to mnist_cnn_rmvcls0_rmvr0.hdf5\n",
      "60000/60000 [==============================] - 5s 78us/step - loss: 0.0732 - acc: 0.9780 - val_loss: 0.1107 - val_acc: 0.9674\n",
      "Epoch 4/50\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0601 - acc: 0.9825Epoch 00004: val_loss did not improve\n",
      "60000/60000 [==============================] - 5s 78us/step - loss: 0.0601 - acc: 0.9826 - val_loss: 0.1320 - val_acc: 0.9658\n",
      "Epoch 5/50\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0525 - acc: 0.9847Epoch 00005: val_loss did not improve\n",
      "60000/60000 [==============================] - 5s 78us/step - loss: 0.0527 - acc: 0.9847 - val_loss: 0.1456 - val_acc: 0.9643\n",
      "Epoch 6/50\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0492 - acc: 0.9849Epoch 00006: val_loss did not improve\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 0.0492 - acc: 0.9850 - val_loss: 0.1598 - val_acc: 0.9621\n",
      "Epoch 7/50\n",
      "59264/60000 [============================>.] - ETA: 0s - loss: 0.0437 - acc: 0.9866Epoch 00007: val_loss did not improve\n",
      "60000/60000 [==============================] - 5s 78us/step - loss: 0.0436 - acc: 0.9867 - val_loss: 0.1441 - val_acc: 0.9642\n",
      "Epoch 8/50\n",
      "59776/60000 [============================>.] - ETA: 0s - loss: 0.0411 - acc: 0.9878Epoch 00008: val_loss did not improve\n",
      "60000/60000 [==============================] - 5s 78us/step - loss: 0.0411 - acc: 0.9878 - val_loss: 0.1747 - val_acc: 0.9601\n",
      "Epoch 00008: early stopping\n",
      "[0.17465925194210141, 0.96009999999999995, 0.17649801167262097, 0.96208333333333329]\n"
     ]
    }
   ],
   "source": [
    "# Find indices of the class to remove\n",
    "ind = np.where(y_train == rmv_cls)[0]\n",
    "# Pick randomly\n",
    "n_rmv = int(len(ind) * (1 - keep_ratio))\n",
    "np.random.shuffle(ind)\n",
    "del_ind = ind[:n_rmv]\n",
    "\n",
    "if replace:\n",
    "    # Replace the chosen indices with generated samples\n",
    "    x_rg = np.copy(x_train)\n",
    "    x_g = generate_label(g, rmv_cls, n_rmv)\n",
    "    x_rg[del_ind] = x_g\n",
    "    y_rg = y_train\n",
    "else:\n",
    "    x_rg = np.delete(x_train, del_ind, axis=0)\n",
    "    y_rg = np.delete(y_train, del_ind, axis=0)\n",
    "    \n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(28, 28, 1)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "if dropout:\n",
    "    model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "if dropout:\n",
    "    model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.sparse_categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto')\n",
    "weight = 'mnist_cnn_rmvcls{}_rmvr{}.hdf5'.format(rmv_cls, keep_ratio)\n",
    "saveBestModel = ModelCheckpoint(weight, monitor='val_loss', verbose=1, \n",
    "                                save_best_only=True, mode='auto')\n",
    "\n",
    "model.fit(x_rg, y_rg,\n",
    "          batch_size=128,\n",
    "          epochs=50,\n",
    "          verbose=1,\n",
    "          callbacks=[earlyStopping, saveBestModel],\n",
    "          validation_data=(x_test, y_test))\n",
    "score_test = model.evaluate(x_test, y_test, verbose=0)\n",
    "score_train = model.evaluate(x_train, y_train, verbose=0)\n",
    "score = [score_test[0], score_test[1], score_train[0], score_train[1]]\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.517620780516644, 0.68469387767266254]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = np.where(y_test == rmv_cls)[0]\n",
    "model.evaluate(x_test[ind], y_test[ind], verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pickle as pickle\n",
    "from PIL import Image\n",
    "from six.moves import range\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from keras.datasets import cifar10\n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Embedding, Dropout, BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import Conv2DTranspose, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.initializers import TruncatedNormal\n",
    "from keras.utils.generic_utils import Progbar\n",
    "from Minibatch import MinibatchDiscrimination\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers.noise import GaussianNoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(latent_size):\n",
    "    # we will map a pair of (z, L), where z is a latent vector and L is a\n",
    "    # label drawn from P_c, to image space (..., 3, 32, 32)\n",
    "    cnn = Sequential()\n",
    "    cnn.add(Dense(384 * 4 * 4, input_dim=latent_size, activation='relu',\n",
    "                  kernel_initializer='TruncatedNormal', bias_initializer='Zeros'))\n",
    "    cnn.add(Reshape((384, 4, 4)))\n",
    "\n",
    "    cnn.add(Conv2DTranspose(192, kernel_size=5, strides=2, padding='same', activation='relu',\n",
    "                            kernel_initializer='TruncatedNormal', bias_initializer='Zeros'))\n",
    "    cnn.add(BatchNormalization())\n",
    "\n",
    "    cnn.add(Conv2DTranspose(96, kernel_size=5, strides=2, padding='same', activation='relu',\n",
    "                            kernel_initializer='TruncatedNormal', bias_initializer='Zeros'))\n",
    "    cnn.add(BatchNormalization())\n",
    "\n",
    "    cnn.add(Conv2DTranspose(3, kernel_size=5, strides=2, padding='same', activation='tanh',\n",
    "                            kernel_initializer='TruncatedNormal', bias_initializer='Zeros'))\n",
    "\n",
    "    # this is the z space commonly refered to in GAN papers\n",
    "    latent = Input(shape=(latent_size, ))\n",
    "\n",
    "    # this will be our label\n",
    "    image_class = Input(shape=(1,), dtype='int32')\n",
    "\n",
    "    # 10 classes in CIFAR-10\n",
    "    cls = Flatten()(Embedding(10, latent_size,\n",
    "                              embeddings_initializer='TruncatedNormal')(image_class))\n",
    "\n",
    "    # hadamard product between z-space and a class conditional embedding\n",
    "    h = layers.multiply([latent, cls])\n",
    "\n",
    "    fake_image = cnn(h)\n",
    "\n",
    "    return Model([latent, image_class], fake_image)\n",
    "\n",
    "\n",
    "def build_discriminator():\n",
    "    # build a relatively standard conv net, with LeakyReLUs as suggested in\n",
    "    # the reference paper\n",
    "    cnn = Sequential()\n",
    "\n",
    "    cnn.add(GaussianNoise(0.05, input_shape=(3, 32, 32))) #Add this layer to prevent D from overfitting!\n",
    "\n",
    "    cnn.add(Conv2D(16, kernel_size=3, strides=2, padding='same',\n",
    "                   kernel_initializer='TruncatedNormal', bias_initializer='Zeros'))\n",
    "    cnn.add(LeakyReLU(alpha=0.2))\n",
    "    cnn.add(Dropout(0.5))\n",
    "\n",
    "    cnn.add(Conv2D(32, kernel_size=3, strides=1, padding='same',\n",
    "                   kernel_initializer='TruncatedNormal', bias_initializer='Zeros'))\n",
    "    cnn.add(BatchNormalization())\n",
    "    cnn.add(LeakyReLU(alpha=0.2))\n",
    "    cnn.add(Dropout(0.5))\n",
    "\n",
    "    cnn.add(Conv2D(64, kernel_size=3, strides=2, padding='same',\n",
    "                   kernel_initializer='TruncatedNormal', bias_initializer='Zeros'))\n",
    "    cnn.add(BatchNormalization())\n",
    "    cnn.add(LeakyReLU(alpha=0.2))\n",
    "    cnn.add(Dropout(0.5))\n",
    "\n",
    "    cnn.add(Conv2D(128, kernel_size=3, strides=1, padding='same',\n",
    "                   kernel_initializer='TruncatedNormal', bias_initializer='Zeros'))\n",
    "    cnn.add(BatchNormalization())\n",
    "    cnn.add(LeakyReLU(alpha=0.2))\n",
    "    cnn.add(Dropout(0.5))\n",
    "\n",
    "    cnn.add(Conv2D(256, kernel_size=3, strides=2, padding='same',\n",
    "                   kernel_initializer='TruncatedNormal', bias_initializer='Zeros'))\n",
    "    cnn.add(BatchNormalization())\n",
    "    cnn.add(LeakyReLU(alpha=0.2))\n",
    "    cnn.add(Dropout(0.5))\n",
    "\n",
    "    cnn.add(Conv2D(512, kernel_size=3, strides=1, padding='same',\n",
    "                   kernel_initializer='TruncatedNormal', bias_initializer='Zeros'))\n",
    "    cnn.add(BatchNormalization())\n",
    "    cnn.add(LeakyReLU(alpha=0.2))\n",
    "    cnn.add(Dropout(0.5))\n",
    "\n",
    "    cnn.add(Flatten())\n",
    "\n",
    "    cnn.add(MinibatchDiscrimination(50, 30))\n",
    "\n",
    "    image = Input(shape=(3, 32, 32))\n",
    "\n",
    "    features = cnn(image)\n",
    "\n",
    "    # first output (name=generation) is whether or not the discriminator\n",
    "    # thinks the image that is being shown is fake, and the second output\n",
    "    # (name=auxiliary) is the class that the discriminator thinks the image\n",
    "    # belongs to.\n",
    "    fake = Dense(1, activation='sigmoid', name='generation',\n",
    "                 kernel_initializer='TruncatedNormal', bias_initializer='Zeros')(features)\n",
    "    aux = Dense(10, activation='softmax', name='auxiliary',\n",
    "                kernel_initializer='TruncatedNormal', bias_initializer='Zeros')(features)\n",
    "\n",
    "    return Model(image, [fake, aux])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sample(size):\n",
    "\n",
    "    # Generate samples from g\n",
    "    z = np.random.normal(0, 0.5, (size, 110))\n",
    "    # Sampled labels\n",
    "    y_sampled = np.random.randint(0, 10, size)\n",
    "    return z, y_sampled\n",
    "\n",
    "def generate_random(g, size):\n",
    "\n",
    "    z, y_sampled = random_sample(size)\n",
    "    x_g = g.predict([z, y_sampled.reshape((-1, 1))], verbose=0)\n",
    "    return x_g, y_sampled\n",
    "\n",
    "def generate_label(g, label, size):\n",
    "    z = np.random.uniform(-1, 1, (size, LATENT_SIZE))\n",
    "    y = np.zeros(size) + label\n",
    "    return g.predict([z, y.reshape((-1, 1))], verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.set_image_dim_ordering('th')\n",
    "\n",
    "latent_size = 110\n",
    "\n",
    "# build the discriminator and generator\n",
    "d = build_discriminator()\n",
    "g = build_generator(latent_size)\n",
    "\n",
    "d.load_weights(\"./Keras-ACGAN-CIFAR10/params_discriminator_epoch_063.hdf5\")\n",
    "g.load_weights(\"./Keras-ACGAN-CIFAR10/params_generator_epoch_063.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = load_cifar10()\n",
    "x_train, x_test = x_train.squeeze(), x_test.squeeze()\n",
    "y_train, y_test = y_train.squeeze(), y_test.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_r = 0.002\n",
    "r_g = 2\n",
    "dropout = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100100 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "100032/100100 [============================>.] - ETA: 0s - loss: 0.8079 - acc: 0.7247Epoch 00001: val_loss improved from inf to 2.25665, saving model to cifar10_cnn_r0.002_g2.hdf5\n",
      "100100/100100 [==============================] - 49s 489us/step - loss: 0.8075 - acc: 0.7248 - val_loss: 2.2566 - val_acc: 0.4184\n",
      "Epoch 2/100\n",
      "100032/100100 [============================>.] - ETA: 0s - loss: 0.2075 - acc: 0.9312Epoch 00002: val_loss did not improve\n",
      "100100/100100 [==============================] - 48s 475us/step - loss: 0.2075 - acc: 0.9312 - val_loss: 2.5594 - val_acc: 0.4460\n",
      "Epoch 3/100\n",
      "100000/100100 [============================>.] - ETA: 0s - loss: 0.1129 - acc: 0.9627Epoch 00003: val_loss did not improve\n",
      "100100/100100 [==============================] - 47s 474us/step - loss: 0.1128 - acc: 0.9627 - val_loss: 2.6547 - val_acc: 0.4589\n",
      "Epoch 4/100\n",
      "100064/100100 [============================>.] - ETA: 0s - loss: 0.0778 - acc: 0.9746Epoch 00004: val_loss did not improve\n",
      "100100/100100 [==============================] - 48s 476us/step - loss: 0.0778 - acc: 0.9746 - val_loss: 2.7934 - val_acc: 0.4632\n",
      "Epoch 5/100\n",
      "100000/100100 [============================>.] - ETA: 0s - loss: 0.0621 - acc: 0.9798Epoch 00005: val_loss did not improve\n",
      "100100/100100 [==============================] - 47s 473us/step - loss: 0.0620 - acc: 0.9798 - val_loss: 3.0770 - val_acc: 0.4652\n",
      "Epoch 6/100\n",
      "100032/100100 [============================>.] - ETA: 0s - loss: 0.0512 - acc: 0.9835Epoch 00006: val_loss did not improve\n",
      "100100/100100 [==============================] - 48s 475us/step - loss: 0.0511 - acc: 0.9835 - val_loss: 2.5721 - val_acc: 0.4672\n",
      "Epoch 00006: early stopping\n",
      "[2.5720944866180422, 0.4672, 2.5579320460510253, 0.47255999999999998]\n"
     ]
    }
   ],
   "source": [
    "n_train, n_test = len(x_train), len(x_test)\n",
    "\n",
    "ind = np.random.choice(n_train, int(n_train * r_r))\n",
    "x_r, y_r = x_train[ind], y_train[ind]\n",
    "x_g, y_g = generate_random(g, int(n_train * r_g))\n",
    "if r_g == 0:\n",
    "    x_rg, y_rg = x_r, y_r\n",
    "elif r_r == 0:\n",
    "    x_rg, y_rg = x_g, y_g\n",
    "else:\n",
    "    x_rg = np.concatenate((x_r, x_g), axis=0)\n",
    "    y_rg = np.concatenate((y_r, y_g), axis=0)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "if dropout:\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "if dropout:\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "if dropout:\n",
    "    model.add(Dropout(0.5))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto')\n",
    "weight = 'cifar10_cnn_r{}_g{}.hdf5'.format(r_r, r_g)\n",
    "saveBestModel = ModelCheckpoint(weight, monitor='val_loss', verbose=1, \n",
    "                                save_best_only=True, mode='auto')\n",
    "\n",
    "model.fit(x_rg, y_rg,\n",
    "          batch_size=32,\n",
    "          epochs=100,\n",
    "          callbacks=[earlyStopping, saveBestModel],\n",
    "          validation_data=(x_test, y_test),\n",
    "          shuffle=True)\n",
    "\n",
    "score_test = model.evaluate(x_test, y_test, verbose=0)\n",
    "score_train = model.evaluate(x_train, y_train, verbose=0)\n",
    "score = [score_test[0], score_test[1], score_train[0], score_train[1]]\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmv_cls = 0\n",
    "keep_ratio = 0\n",
    "replace = True\n",
    "dropout = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "49888/50000 [============================>.] - ETA: 0s - loss: 1.6943 - acc: 0.3772Epoch 00001: val_loss improved from inf to 1.49670, saving model to cifar10_cnn_rmvcls0_rmvr0.hdf5\n",
      "50000/50000 [==============================] - 25s 494us/step - loss: 1.6938 - acc: 0.3773 - val_loss: 1.4967 - val_acc: 0.4607\n",
      "Epoch 2/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 1.3450 - acc: 0.5123Epoch 00002: val_loss improved from 1.49670 to 1.35523, saving model to cifar10_cnn_rmvcls0_rmvr0.hdf5\n",
      "50000/50000 [==============================] - 24s 482us/step - loss: 1.3448 - acc: 0.5124 - val_loss: 1.3552 - val_acc: 0.5167\n",
      "Epoch 3/100\n",
      "49952/50000 [============================>.] - ETA: 0s - loss: 1.1987 - acc: 0.5693Epoch 00003: val_loss improved from 1.35523 to 1.28769, saving model to cifar10_cnn_rmvcls0_rmvr0.hdf5\n",
      "50000/50000 [==============================] - 24s 481us/step - loss: 1.1985 - acc: 0.5694 - val_loss: 1.2877 - val_acc: 0.5593\n",
      "Epoch 4/100\n",
      "49888/50000 [============================>.] - ETA: 0s - loss: 1.0910 - acc: 0.6128Epoch 00004: val_loss improved from 1.28769 to 1.21945, saving model to cifar10_cnn_rmvcls0_rmvr0.hdf5\n",
      "50000/50000 [==============================] - 24s 482us/step - loss: 1.0906 - acc: 0.6129 - val_loss: 1.2194 - val_acc: 0.5881\n",
      "Epoch 5/100\n",
      "49952/50000 [============================>.] - ETA: 0s - loss: 1.0070 - acc: 0.6404Epoch 00005: val_loss did not improve\n",
      "50000/50000 [==============================] - 24s 479us/step - loss: 1.0071 - acc: 0.6404 - val_loss: 1.3445 - val_acc: 0.5923\n",
      "Epoch 6/100\n",
      "49888/50000 [============================>.] - ETA: 0s - loss: 0.9418 - acc: 0.6661Epoch 00006: val_loss did not improve\n",
      "50000/50000 [==============================] - 24s 481us/step - loss: 0.9420 - acc: 0.6660 - val_loss: 1.2985 - val_acc: 0.6104\n",
      "Epoch 7/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.8843 - acc: 0.6858Epoch 00007: val_loss improved from 1.21945 to 1.18018, saving model to cifar10_cnn_rmvcls0_rmvr0.hdf5\n",
      "50000/50000 [==============================] - 24s 482us/step - loss: 0.8844 - acc: 0.6858 - val_loss: 1.1802 - val_acc: 0.6320\n",
      "Epoch 8/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.8348 - acc: 0.7057Epoch 00008: val_loss did not improve\n",
      "50000/50000 [==============================] - 24s 479us/step - loss: 0.8347 - acc: 0.7058 - val_loss: 1.4232 - val_acc: 0.6253\n",
      "Epoch 9/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.7987 - acc: 0.7169Epoch 00009: val_loss did not improve\n",
      "50000/50000 [==============================] - 24s 480us/step - loss: 0.7987 - acc: 0.7169 - val_loss: 1.3250 - val_acc: 0.6438\n",
      "Epoch 10/100\n",
      "49984/50000 [============================>.] - ETA: 0s - loss: 0.7657 - acc: 0.7317Epoch 00010: val_loss did not improve\n",
      "50000/50000 [==============================] - 24s 483us/step - loss: 0.7657 - acc: 0.7316 - val_loss: 1.3468 - val_acc: 0.6498\n",
      "Epoch 11/100\n",
      "49952/50000 [============================>.] - ETA: 0s - loss: 0.7402 - acc: 0.7408Epoch 00011: val_loss did not improve\n",
      "50000/50000 [==============================] - 24s 482us/step - loss: 0.7402 - acc: 0.7408 - val_loss: 1.3386 - val_acc: 0.6540\n",
      "Epoch 12/100\n",
      "49920/50000 [============================>.] - ETA: 0s - loss: 0.7171 - acc: 0.7487Epoch 00012: val_loss did not improve\n",
      "50000/50000 [==============================] - 24s 482us/step - loss: 0.7170 - acc: 0.7487 - val_loss: 1.3587 - val_acc: 0.6486\n",
      "Epoch 00012: early stopping\n",
      "[1.3586812580108643, 0.64859999999999995, 1.2728546558952332, 0.67732000000000003]\n"
     ]
    }
   ],
   "source": [
    "# Find indices of the class to remove\n",
    "ind = np.where(y_train == rmv_cls)[0]\n",
    "# Pick randomly\n",
    "n_rmv = int(len(ind) * (1 - keep_ratio))\n",
    "np.random.shuffle(ind)\n",
    "del_ind = ind[:n_rmv]\n",
    "\n",
    "if replace:\n",
    "    # Replace the chosen indices with generated samples\n",
    "    x_rg = np.copy(x_train)\n",
    "    x_g = generate_label(g, rmv_cls, n_rmv)\n",
    "    x_rg[del_ind] = x_g\n",
    "    y_rg = y_train\n",
    "else:\n",
    "    x_rg = np.delete(x_train, del_ind, axis=0)\n",
    "    y_rg = np.delete(y_train, del_ind, axis=0)\n",
    "    \n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto')\n",
    "weight = 'cifar10_cnn_rmvcls{}_rmvr{}.hdf5'.format(rmv_cls, keep_ratio)\n",
    "saveBestModel = ModelCheckpoint(weight, monitor='val_loss', verbose=1, \n",
    "                                save_best_only=True, mode='auto')\n",
    "\n",
    "model.fit(x_rg, y_rg,\n",
    "          batch_size=32,\n",
    "          epochs=100,\n",
    "          callbacks=[earlyStopping, saveBestModel],\n",
    "          validation_data=(x_test, y_test),\n",
    "          shuffle=True)\n",
    "\n",
    "score_test = model.evaluate(x_test, y_test, verbose=0)\n",
    "score_train = model.evaluate(x_train, y_train, verbose=0)\n",
    "score = [score_test[0], score_test[1], score_train[0], score_train[1]]\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6.0985440711975096, 0.112]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = np.where(y_test == rmv_cls)[0]\n",
    "model.evaluate(x_test[ind], y_test[ind], verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.model_acgan_cifar10 import *\n",
    "\n",
    "def train(prog=True):\n",
    "    \"\"\"Main function to train GAN\"\"\"\n",
    "\n",
    "    # Load MNIST\n",
    "    x_train, y_train, x_test, y_test = load_cifar10()\n",
    "    x_train, x_test = x_train.squeeze(), x_test.squeeze()\n",
    "    y_train, y_test = y_train.squeeze(), y_test.squeeze()\n",
    "\n",
    "    # Build model\n",
    "    d = build_discriminator()\n",
    "    g = build_generator()\n",
    "\n",
    "    # Set up optimizers\n",
    "    adam = Adam(lr=adam_lr, beta_1=adam_beta_1)\n",
    "\n",
    "    # Set loss function and compile models\n",
    "    g.compile(optimizer=adam, loss='binary_crossentropy')\n",
    "    d.compile(\n",
    "        optimizer=adam,\n",
    "        loss=['binary_crossentropy', 'sparse_categorical_crossentropy'])\n",
    "    combined = combine_g_d(g, d)\n",
    "    combined.compile(\n",
    "        optimizer=adam,\n",
    "        loss=['binary_crossentropy', 'sparse_categorical_crossentropy'])\n",
    "\n",
    "    train_history = defaultdict(list)\n",
    "    test_history = defaultdict(list)\n",
    "\n",
    "    n_train = x_train.shape[0]\n",
    "    n_batch = int(n_train / BATCH_SIZE)\n",
    "    for epoch in range(N_EPOCH):\n",
    "        print('Epoch {} of {}'.format(epoch + 1, N_EPOCH))\n",
    "        progress_bar = Progbar(target=n_batch)\n",
    "\n",
    "        epoch_g_loss = []\n",
    "        epoch_d_loss = []\n",
    "\n",
    "        for index in range(n_batch):\n",
    "            progress_bar.update(index, force=True)\n",
    "            d_loss = np.zeros(3)\n",
    "\n",
    "            # Train the discriminator for N_DIS iterations before training\n",
    "            # the generator once\n",
    "            for _ in range(N_DIS):\n",
    "                # ----------------- Train discriminator ---------------------- #\n",
    "                # Train with real samples first\n",
    "                smp_ind = np.random.choice(n_train, BATCH_SIZE)\n",
    "                x_real = x_train[smp_ind]\n",
    "                y_real = y_train[smp_ind]\n",
    "                if index % 30 != 0:\n",
    "                    y_d = np.random.uniform(0.7, 1.2, size=(BATCH_SIZE, ))\n",
    "                else:\n",
    "                    y_d = np.random.uniform(0.0, 0.3, size=(BATCH_SIZE, ))\n",
    "                d_loss += d.train_on_batch(x_real, [y_d, y_real])\n",
    "\n",
    "                # Train with generated samples, generate samples from g\n",
    "                z = np.random.normal(0, 0.5, (BATCH_SIZE, LATENT_SIZE))\n",
    "                # Sample some labels from p_c\n",
    "                y_sampled = np.random.randint(0, 10, BATCH_SIZE)\n",
    "                x_g = g.predict([z, y_sampled.reshape((-1, 1))], verbose=0)\n",
    "                if index % 30 != 0:\n",
    "                    y_d = np.random.uniform(0.0, 0.3, size=(BATCH_SIZE, ))\n",
    "                else:\n",
    "                    y_d = np.random.uniform(0.7, 1.2, size=(BATCH_SIZE, ))\n",
    "                d_loss += d.train_on_batch(x_g, [y_d, y_sampled])\n",
    "\n",
    "            # Log average discriminator loss over N_DIS\n",
    "            epoch_d_loss.append(d_loss / N_DIS)\n",
    "\n",
    "            # ---------------- Train generator ------------------------------- #\n",
    "            # Generate 2 * BATCH_SIZE samples to match d's batch size\n",
    "            z = np.random.uniform(0, 0.5, (2 * BATCH_SIZE, LATENT_SIZE))\n",
    "            y_sampled = np.random.randint(0, 10, 2 * BATCH_SIZE)\n",
    "            y_g = np.random.uniform(0.7, 1.2, size=(2 * BATCH_SIZE, ))\n",
    "\n",
    "            epoch_g_loss.append(combined.train_on_batch(\n",
    "                [z, y_sampled.reshape((-1, 1))], [y_g, y_sampled]))\n",
    "\n",
    "        print('\\nTesting for epoch {}:'.format(epoch + 1))\n",
    "        n_test = x_test.shape[0]\n",
    "\n",
    "        # ---------------- Test discriminator -------------------------------- #\n",
    "        z = np.random.uniform(0, 0.5, (n_test, LATENT_SIZE))\n",
    "        y_sampled = np.random.randint(0, 10, n_test)\n",
    "        x_g = g.predict([z, y_sampled.reshape((-1, 1))], verbose=0)\n",
    "\n",
    "        x_d = np.concatenate((x_test, x_g))\n",
    "        y_d = np.array([1] * n_test + [0] * n_test)\n",
    "        y_aux = np.concatenate((y_test, y_sampled), axis=0)\n",
    "\n",
    "        d_test_loss = d.evaluate(x_d, [y_d, y_aux], verbose=0)\n",
    "        d_train_loss = np.mean(np.array(epoch_d_loss), axis=0)\n",
    "\n",
    "        # ---------------- Test generator ------------------------------------ #\n",
    "        z = np.random.uniform(0, 0.5, (2 * n_test, LATENT_SIZE))\n",
    "        y_sampled = np.random.randint(0, 10, 2 * n_test)\n",
    "        y_g = np.ones(2 * n_test)\n",
    "\n",
    "        g_test_loss = combined.evaluate(\n",
    "            [z, y_sampled.reshape((-1, 1))], [y_g, y_sampled], verbose=0)\n",
    "        g_train_loss = np.mean(np.array(epoch_g_loss), axis=0)\n",
    "\n",
    "        # generate an epoch report on performance\n",
    "        train_history['generator'].append(g_train_loss)\n",
    "        train_history['discriminator'].append(d_train_loss)\n",
    "        test_history['generator'].append(g_test_loss)\n",
    "        test_history['discriminator'].append(d_test_loss)\n",
    "\n",
    "        print('{0:<22s} | {1:4s} | {2:15s} | {3:5s}'.format(\n",
    "            'component', *d.metrics_names))\n",
    "        print('-' * 65)\n",
    "\n",
    "        ROW_FMT = '{0:<22s} | {1:<4.2f} | {2:<15.2f} | {3:<5.2f}'\n",
    "        print(ROW_FMT.format('generator (train)',\n",
    "                             *train_history['generator'][-1]))\n",
    "        print(ROW_FMT.format('generator (test)',\n",
    "                             *test_history['generator'][-1]))\n",
    "        print(ROW_FMT.format('discriminator (train)',\n",
    "                             *train_history['discriminator'][-1]))\n",
    "        print(ROW_FMT.format('discriminator (test)',\n",
    "                             *test_history['discriminator'][-1]))\n",
    "\n",
    "        # Aave weights every epoch\n",
    "        g.save_weights(\"{}weight_g_epoch_{:03d}.hdf5\".format(\n",
    "            WEIGHT_DIR, epoch), True)\n",
    "        d.save_weights(\"{}weight_d_epoch_{:03d}.hdf5\".format(\n",
    "            WEIGHT_DIR, epoch), True)\n",
    "\n",
    "        # generate some digits to display\n",
    "        noise = np.random.uniform(0, 0.5, (100, LATENT_SIZE))\n",
    "\n",
    "        sampled_labels = np.array([\n",
    "            [i] * 10 for i in range(10)\n",
    "        ]).reshape(-1, 1)\n",
    "\n",
    "        # get a batch to display\n",
    "        generated_images = g.predict(\n",
    "            [noise, sampled_labels], verbose=0)\n",
    "\n",
    "        def vis_square(data, padsize=1, padval=-1):\n",
    "\n",
    "            # force the number of filters to be square\n",
    "            n = int(np.ceil(np.sqrt(data.shape[0])))\n",
    "            padding = ((0, n ** 2 - data.shape[0]), (0, padsize), \n",
    "                       (0, padsize)) + ((0, 0),) * (data.ndim - 3)\n",
    "            data = np.pad(data, padding, mode='constant', \n",
    "                          constant_values=(padval, padval))\n",
    "\n",
    "            # tile the filters into an image\n",
    "            data = data.reshape((n, n) + data.shape[1:]).transpose(\n",
    "                (0, 2, 1, 3) + tuple(range(4, data.ndim + 1)))\n",
    "            data = data.reshape((n * data.shape[1], n * data.shape[3]) + data.shape[4:])\n",
    "    \n",
    "            return (data * SCALE + SCALE).astype(np.uint8)\n",
    "        \n",
    "        img = vis_square(generated_images)\n",
    "\n",
    "        Image.fromarray(img).save(\n",
    "            '{}plot_epoch_{:03d}_generated.png'.format(VIS_DIR, epoch))\n",
    "\n",
    "    pickle.dump({'train': train_history, 'test': test_history},\n",
    "                open('wgan-history.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 100\n",
      "\r",
      "  0/500 [..............................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chawins/.conda/envs/tsa3/lib/python3.6/site-packages/keras/engine/training.py:973: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499/500 [============================>.] - ETA: 0s\n",
      "Testing for epoch 1:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 2.16 | 0.37            | 1.79 \n",
      "generator (test)       | 2.51 | 0.56            | 1.95 \n",
      "discriminator (train)  | 7.95 | 1.23            | 6.72 \n",
      "discriminator (test)   | 2.65 | 0.52            | 2.13 \n",
      "Epoch 2 of 100\n",
      "499/500 [============================>.] - ETA: 0s\n",
      "Testing for epoch 2:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 1.16 | 0.31            | 0.85 \n",
      "generator (test)       | 1.20 | 0.35            | 0.85 \n",
      "discriminator (train)  | 5.43 | 0.88            | 4.55 \n",
      "discriminator (test)   | 2.43 | 0.94            | 1.49 \n",
      "Epoch 3 of 100\n",
      "499/500 [============================>.] - ETA: 0s\n",
      "Testing for epoch 3:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 0.75 | 0.32            | 0.42 \n",
      "generator (test)       | 0.71 | 0.28            | 0.42 \n",
      "discriminator (train)  | 4.91 | 0.83            | 4.08 \n",
      "discriminator (test)   | 2.44 | 0.96            | 1.48 \n",
      "Epoch 4 of 100\n",
      "499/500 [============================>.] - ETA: 0s\n",
      "Testing for epoch 4:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 0.61 | 0.26            | 0.35 \n",
      "generator (test)       | 0.69 | 0.37            | 0.32 \n",
      "discriminator (train)  | 4.49 | 0.79            | 3.70 \n",
      "discriminator (test)   | 2.32 | 0.83            | 1.49 \n",
      "Epoch 5 of 100\n",
      "499/500 [============================>.] - ETA: 0s\n",
      "Testing for epoch 5:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 0.58 | 0.33            | 0.25 \n",
      "generator (test)       | 0.59 | 0.28            | 0.31 \n",
      "discriminator (train)  | 4.10 | 0.77            | 3.33 \n",
      "discriminator (test)   | 2.14 | 0.94            | 1.20 \n",
      "Epoch 6 of 100\n",
      "499/500 [============================>.] - ETA: 0s\n",
      "Testing for epoch 6:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 0.50 | 0.31            | 0.20 \n",
      "generator (test)       | 0.55 | 0.21            | 0.34 \n",
      "discriminator (train)  | 4.05 | 0.78            | 3.27 \n",
      "discriminator (test)   | 2.29 | 1.03            | 1.26 \n",
      "Epoch 7 of 100\n",
      "499/500 [============================>.] - ETA: 0s\n",
      "Testing for epoch 7:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 0.32 | 0.26            | 0.06 \n",
      "generator (test)       | 0.75 | 0.35            | 0.40 \n",
      "discriminator (train)  | 4.03 | 0.77            | 3.26 \n",
      "discriminator (test)   | 2.03 | 0.82            | 1.21 \n",
      "Epoch 8 of 100\n",
      "499/500 [============================>.] - ETA: 0s\n",
      "Testing for epoch 8:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 0.29 | 0.25            | 0.04 \n",
      "generator (test)       | 0.42 | 0.23            | 0.19 \n",
      "discriminator (train)  | 3.95 | 0.76            | 3.20 \n",
      "discriminator (test)   | 1.91 | 0.99            | 0.91 \n",
      "Epoch 9 of 100\n",
      "499/500 [============================>.] - ETA: 0s\n",
      "Testing for epoch 9:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 0.29 | 0.26            | 0.04 \n",
      "generator (test)       | 0.56 | 0.34            | 0.22 \n",
      "discriminator (train)  | 3.92 | 0.77            | 3.16 \n",
      "discriminator (test)   | 1.76 | 0.82            | 0.94 \n",
      "Epoch 10 of 100\n",
      "499/500 [============================>.] - ETA: 0s\n",
      "Testing for epoch 10:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 0.27 | 0.24            | 0.02 \n",
      "generator (test)       | 0.37 | 0.25            | 0.12 \n",
      "discriminator (train)  | 3.85 | 0.76            | 3.09 \n",
      "discriminator (test)   | 1.66 | 0.89            | 0.77 \n",
      "Epoch 11 of 100\n",
      "499/500 [============================>.] - ETA: 0s\n",
      "Testing for epoch 11:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 0.25 | 0.23            | 0.02 \n",
      "generator (test)       | 0.38 | 0.28            | 0.11 \n",
      "discriminator (train)  | 3.78 | 0.77            | 3.02 \n",
      "discriminator (test)   | 1.58 | 0.84            | 0.74 \n",
      "Epoch 12 of 100\n",
      "499/500 [============================>.] - ETA: 0s\n",
      "Testing for epoch 12:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 0.26 | 0.24            | 0.02 \n",
      "generator (test)       | 0.38 | 0.27            | 0.10 \n",
      "discriminator (train)  | 3.75 | 0.76            | 2.99 \n",
      "discriminator (test)   | 1.57 | 0.83            | 0.74 \n",
      "Epoch 13 of 100\n",
      "499/500 [============================>.] - ETA: 0s\n",
      "Testing for epoch 13:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 0.24 | 0.23            | 0.01 \n",
      "generator (test)       | 0.30 | 0.23            | 0.07 \n",
      "discriminator (train)  | 3.68 | 0.76            | 2.93 \n",
      "discriminator (test)   | 1.57 | 0.89            | 0.68 \n",
      "Epoch 14 of 100\n",
      "499/500 [============================>.] - ETA: 0s\n",
      "Testing for epoch 14:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 0.24 | 0.23            | 0.02 \n",
      "generator (test)       | 0.34 | 0.24            | 0.09 \n",
      "discriminator (train)  | 3.63 | 0.76            | 2.87 \n",
      "discriminator (test)   | 1.55 | 0.88            | 0.67 \n",
      "Epoch 15 of 100\n",
      "499/500 [============================>.] - ETA: 0s\n",
      "Testing for epoch 15:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 0.23 | 0.22            | 0.01 \n",
      "generator (test)       | 0.29 | 0.23            | 0.07 \n",
      "discriminator (train)  | 3.62 | 0.76            | 2.86 \n",
      "discriminator (test)   | 1.57 | 0.92            | 0.65 \n",
      "Epoch 16 of 100\n",
      "499/500 [============================>.] - ETA: 0s\n",
      "Testing for epoch 16:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 0.23 | 0.22            | 0.01 \n",
      "generator (test)       | 0.37 | 0.29            | 0.08 \n",
      "discriminator (train)  | 3.59 | 0.76            | 2.83 \n",
      "discriminator (test)   | 1.49 | 0.82            | 0.67 \n",
      "Epoch 17 of 100\n",
      "499/500 [============================>.] - ETA: 0s\n",
      "Testing for epoch 17:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 0.24 | 0.23            | 0.01 \n",
      "generator (test)       | 0.40 | 0.30            | 0.10 \n",
      "discriminator (train)  | 3.59 | 0.76            | 2.83 \n",
      "discriminator (test)   | 1.44 | 0.79            | 0.65 \n",
      "Epoch 18 of 100\n",
      "499/500 [============================>.] - ETA: 0s\n",
      "Testing for epoch 18:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 0.23 | 0.22            | 0.01 \n",
      "generator (test)       | 0.32 | 0.25            | 0.07 \n",
      "discriminator (train)  | 3.55 | 0.76            | 2.79 \n",
      "discriminator (test)   | 1.50 | 0.87            | 0.63 \n",
      "Epoch 19 of 100\n",
      "499/500 [============================>.] - ETA: 0s\n",
      "Testing for epoch 19:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 0.23 | 0.22            | 0.01 \n",
      "generator (test)       | 0.29 | 0.24            | 0.06 \n",
      "discriminator (train)  | 3.52 | 0.76            | 2.76 \n",
      "discriminator (test)   | 1.55 | 0.92            | 0.63 \n",
      "Epoch 20 of 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499/500 [============================>.] - ETA: 0s\n",
      "Testing for epoch 20:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 0.22 | 0.21            | 0.01 \n",
      "generator (test)       | 0.31 | 0.25            | 0.06 \n",
      "discriminator (train)  | 3.46 | 0.76            | 2.70 \n",
      "discriminator (test)   | 1.50 | 0.86            | 0.64 \n",
      "Epoch 21 of 100\n",
      "499/500 [============================>.] - ETA: 0s\n",
      "Testing for epoch 21:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 0.22 | 0.21            | 0.01 \n",
      "generator (test)       | 0.36 | 0.30            | 0.06 \n",
      "discriminator (train)  | 3.42 | 0.76            | 2.67 \n",
      "discriminator (test)   | 1.37 | 0.78            | 0.59 \n",
      "Epoch 22 of 100\n",
      "499/500 [============================>.] - ETA: 0s\n",
      "Testing for epoch 22:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 0.22 | 0.21            | 0.01 \n",
      "generator (test)       | 0.40 | 0.35            | 0.05 \n",
      "discriminator (train)  | 3.37 | 0.76            | 2.61 \n",
      "discriminator (test)   | 1.33 | 0.73            | 0.59 \n",
      "Epoch 23 of 100\n",
      "499/500 [============================>.] - ETA: 0s\n",
      "Testing for epoch 23:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 0.22 | 0.21            | 0.01 \n",
      "generator (test)       | 0.38 | 0.31            | 0.07 \n",
      "discriminator (train)  | 3.36 | 0.76            | 2.60 \n",
      "discriminator (test)   | 1.35 | 0.77            | 0.58 \n",
      "Epoch 24 of 100\n",
      "499/500 [============================>.] - ETA: 0s\n",
      "Testing for epoch 24:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 0.22 | 0.21            | 0.01 \n",
      "generator (test)       | 0.43 | 0.36            | 0.07 \n",
      "discriminator (train)  | 3.32 | 0.76            | 2.56 \n",
      "discriminator (test)   | 1.30 | 0.71            | 0.59 \n",
      "Epoch 25 of 100\n",
      "499/500 [============================>.] - ETA: 0s\n",
      "Testing for epoch 25:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 0.22 | 0.21            | 0.01 \n",
      "generator (test)       | 0.38 | 0.32            | 0.06 \n",
      "discriminator (train)  | 3.31 | 0.76            | 2.55 \n",
      "discriminator (test)   | 1.36 | 0.77            | 0.59 \n",
      "Epoch 26 of 100\n",
      "499/500 [============================>.] - ETA: 0s\n",
      "Testing for epoch 26:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 0.22 | 0.21            | 0.01 \n",
      "generator (test)       | 0.44 | 0.34            | 0.10 \n",
      "discriminator (train)  | 3.29 | 0.76            | 2.54 \n",
      "discriminator (test)   | 1.32 | 0.73            | 0.59 \n",
      "Epoch 27 of 100\n",
      "499/500 [============================>.] - ETA: 0s\n",
      "Testing for epoch 27:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 0.22 | 0.21            | 0.01 \n",
      "generator (test)       | 0.46 | 0.36            | 0.10 \n",
      "discriminator (train)  | 3.28 | 0.76            | 2.52 \n",
      "discriminator (test)   | 1.33 | 0.72            | 0.62 \n",
      "Epoch 28 of 100\n",
      "499/500 [============================>.] - ETA: 0s\n",
      "Testing for epoch 28:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 0.23 | 0.22            | 0.01 \n",
      "generator (test)       | 0.48 | 0.35            | 0.13 \n",
      "discriminator (train)  | 3.37 | 0.76            | 2.61 \n",
      "discriminator (test)   | 1.34 | 0.72            | 0.62 \n",
      "Epoch 29 of 100\n",
      "499/500 [============================>.] - ETA: 0s\n",
      "Testing for epoch 29:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 0.23 | 0.22            | 0.01 \n",
      "generator (test)       | 0.41 | 0.31            | 0.10 \n",
      "discriminator (train)  | 3.29 | 0.75            | 2.54 \n",
      "discriminator (test)   | 1.42 | 0.77            | 0.64 \n",
      "Epoch 30 of 100\n",
      "499/500 [============================>.] - ETA: 0s\n",
      "Testing for epoch 30:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 0.23 | 0.22            | 0.01 \n",
      "generator (test)       | 0.45 | 0.37            | 0.08 \n",
      "discriminator (train)  | 3.24 | 0.76            | 2.49 \n",
      "discriminator (test)   | 1.30 | 0.71            | 0.59 \n",
      "Epoch 31 of 100\n",
      "499/500 [============================>.] - ETA: 0s\n",
      "Testing for epoch 31:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 0.22 | 0.21            | 0.01 \n",
      "generator (test)       | 0.43 | 0.35            | 0.08 \n",
      "discriminator (train)  | 3.21 | 0.76            | 2.45 \n",
      "discriminator (test)   | 1.31 | 0.72            | 0.59 \n",
      "Epoch 32 of 100\n",
      "499/500 [============================>.] - ETA: 0s\n",
      "Testing for epoch 32:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 0.23 | 0.22            | 0.01 \n",
      "generator (test)       | 0.38 | 0.33            | 0.06 \n",
      "discriminator (train)  | 3.19 | 0.75            | 2.44 \n",
      "discriminator (test)   | 1.40 | 0.77            | 0.63 \n",
      "Epoch 33 of 100\n",
      "328/500 [==================>...........] - ETA: 52s"
     ]
    }
   ],
   "source": [
    "train(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_images = np.ones((100, 32, 32, 3))\n",
    "\n",
    "def vis_square(data, padsize=1, padval=0):\n",
    "\n",
    "    # force the number of filters to be square\n",
    "    n = int(np.ceil(np.sqrt(data.shape[0])))\n",
    "    padding = ((0, n ** 2 - data.shape[0]), (0, padsize), (0, padsize)) + ((0, 0),) * (data.ndim - 3)\n",
    "    data = np.pad(data, padding, mode='constant', constant_values=(padval, padval))\n",
    "    \n",
    "    # tile the filters into an image\n",
    "    data = data.reshape((n, n) + data.shape[1:]).transpose(\n",
    "        (0, 2, 1, 3) + tuple(range(4, data.ndim + 1)))\n",
    "    data = data.reshape((n * data.shape[1], n * data.shape[3]) + data.shape[4:])\n",
    "    \n",
    "#     w = data.shape[1]\n",
    "#     tmp = np.zeros((n*w, n*w, data.shape[-1]))\n",
    "#     for i, x in enumerate(data):\n",
    "#         j = int(i / n)\n",
    "#         k = i % n\n",
    "#         tmp[j*w:(j+1)*w, k*w:(k+1)*w, :] = x\n",
    "    \n",
    "    # tile the filters into an image\n",
    "#     data = (np.concatenate([r.reshape(330, 33, 3)\n",
    "#                             for r in np.split(data, 10)\n",
    "#                            ], axis=1) * SCALE + SCALE).astype(np.uint8)\n",
    "    return data\n",
    "        \n",
    "img = vis_square(generated_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fdf6742f898>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQUAAAD8CAYAAAB+fLH0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6hgKkjqGgqSOoSCpYyhI6vwfCoSsTvNJorcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdf674c74a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(123,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(0, 10, 123).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For WGAN, real = -1, fake = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from lib.model_wgan import *\n",
    "from lib.utils import *\n",
    "from param import *\n",
    "\n",
    "# Set CUDA visible device to GPU:0\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Adam parameters suggested in https://arxiv.org/abs/1511.06434\n",
    "adam_lr = 0.0002\n",
    "adam_beta_1 = 0.5\n",
    "rms_lr = 0.00005\n",
    "\n",
    "def train(prog=True):\n",
    "    \"\"\"Main function to train GAN\"\"\"\n",
    "\n",
    "    # Load MNIST\n",
    "    x_train, y_train, x_test, y_test = load_mnist()\n",
    "\n",
    "    # Build model\n",
    "    d = build_discriminator()\n",
    "    g = build_generator()\n",
    "\n",
    "    # Set up optimizers\n",
    "    #adam = Adam(lr=adam_lr, beta_1=adam_beta_1)\n",
    "    rms = RMSprop(lr=rms_lr)\n",
    "    \n",
    "    # Set loss function and compile models\n",
    "    g.compile(optimizer=rms, loss='binary_crossentropy')\n",
    "    d.compile(\n",
    "        optimizer=rms, \n",
    "        loss=[modified_binary_crossentropy, 'sparse_categorical_crossentropy'])\n",
    "    combined = combine_g_d(g, d)\n",
    "    combined.compile(\n",
    "        optimizer=rms,\n",
    "        loss=[modified_binary_crossentropy, 'sparse_categorical_crossentropy'])\n",
    "\n",
    "    train_history = defaultdict(list)\n",
    "    test_history = defaultdict(list)\n",
    "\n",
    "    n_train = x_train.shape[0]\n",
    "    n_batch = int(n_train / BATCH_SIZE)\n",
    "    for epoch in range(N_EPOCH):\n",
    "        print('Epoch {} of {}'.format(epoch + 1, N_EPOCH))\n",
    "        progress_bar = Progbar(target=n_batch)\n",
    "\n",
    "        epoch_g_loss = []\n",
    "        epoch_d_loss = []\n",
    "\n",
    "        for index in range(n_batch):\n",
    "            progress_bar.update(index, force=True)\n",
    "            d_loss = np.zeros(3)\n",
    "\n",
    "            # Train the discriminator for N_DIS iterations before training \n",
    "            # the generator once\n",
    "            for _ in range(N_DIS):\n",
    "                # ----------------- Train discriminator ---------------------- #\n",
    "                # Train with real samples first\n",
    "                smp_ind = np.random.choice(n_train, BATCH_SIZE)\n",
    "                x_real = x_train[smp_ind]\n",
    "                y_real = y_train[smp_ind]\n",
    "                y_d = np.array([-1] * BATCH_SIZE)\n",
    "                d_loss += d.train_on_batch(x_real, [y_d, y_real])\n",
    "                \n",
    "                # Train with generated samples, generate samples from g\n",
    "                z = np.random.normal(0, 1, (BATCH_SIZE, LATENT_SIZE))\n",
    "                # Sample some labels from p_c\n",
    "                y_sampled = np.random.randint(0, 10, BATCH_SIZE)\n",
    "                x_g = g.predict([z, y_sampled.reshape((-1, 1))], verbose=0)\n",
    "                y_d = np.array([1] * BATCH_SIZE)\n",
    "                d_loss += d.train_on_batch(x_g, [y_d, y_sampled])\n",
    "                \n",
    "            # Log average discriminator loss over N_DIS\n",
    "            epoch_d_loss.append(d_loss / N_DIS)\n",
    "\n",
    "            # ---------------- Train generator ------------------------------- #\n",
    "            # Generate 2 * BATCH_SIZE samples to match d's batch size\n",
    "            z = np.random.normal(0, 1, (BATCH_SIZE, LATENT_SIZE))\n",
    "            y_sampled = np.random.randint(0, 10, BATCH_SIZE)\n",
    "            y_g = np.array([-1] * BATCH_SIZE)\n",
    "\n",
    "            epoch_g_loss.append(combined.train_on_batch(\n",
    "                [z, y_sampled.reshape((-1, 1))], [y_g, y_sampled]))\n",
    "\n",
    "        print('\\nTesting for epoch {}:'.format(epoch + 1))\n",
    "        n_test = x_test.shape[0]\n",
    "\n",
    "        # ---------------- Test discriminator -------------------------------- #\n",
    "        z = np.random.normal(0, 1, (n_test, LATENT_SIZE))\n",
    "        y_sampled = np.random.randint(0, 10, n_test)\n",
    "        x_g = g.predict([z, y_sampled.reshape((-1, 1))], verbose=0)\n",
    "\n",
    "        x_d = np.concatenate((x_test, x_g))\n",
    "        y_d = np.array([-1] * n_test + [1] * n_test)\n",
    "        y_aux = np.concatenate((y_test, y_sampled), axis=0)\n",
    "\n",
    "        d_test_loss = d.evaluate(x_d, [y_d, y_aux], verbose=0)\n",
    "        d_train_loss = np.mean(np.array(epoch_d_loss), axis=0)\n",
    "\n",
    "        # ---------------- Test generator ------------------------------------ #\n",
    "        z = np.random.normal(0, 1, (2 * n_test, LATENT_SIZE))\n",
    "        y_sampled = np.random.randint(0, 10, 2 * n_test)\n",
    "        y_g = np.array([-1] * 2 * n_test)\n",
    "\n",
    "        g_test_loss = combined.evaluate(\n",
    "            [z, y_sampled.reshape((-1, 1))], [y_g, y_sampled], verbose=0)\n",
    "        g_train_loss = np.mean(np.array(epoch_g_loss), axis=0)\n",
    "\n",
    "        # generate an epoch report on performance\n",
    "        train_history['generator'].append(g_train_loss)\n",
    "        train_history['discriminator'].append(d_train_loss)\n",
    "        test_history['generator'].append(g_test_loss)\n",
    "        test_history['discriminator'].append(d_test_loss)\n",
    "\n",
    "        print('{0:<22s} | {1:4s} | {2:15s} | {3:5s}'.format(\n",
    "            'component', *d.metrics_names))\n",
    "        print('-' * 65)\n",
    "\n",
    "        ROW_FMT = '{0:<22s} | {1:<4.2f} | {2:<15.2f} | {3:<5.2f}'\n",
    "        print(ROW_FMT.format('generator (train)',\n",
    "                             *train_history['generator'][-1]))\n",
    "        print(ROW_FMT.format('generator (test)',\n",
    "                             *test_history['generator'][-1]))\n",
    "        print(ROW_FMT.format('discriminator (train)',\n",
    "                             *train_history['discriminator'][-1]))\n",
    "        print(ROW_FMT.format('discriminator (test)',\n",
    "                             *test_history['discriminator'][-1]))\n",
    "\n",
    "        # Aave weights every epoch\n",
    "        g.save_weights(\"{}weight_g_epoch_{:03d}.hdf5\".format(\n",
    "            WEIGHT_DIR, epoch), True)\n",
    "        d.save_weights(\"{}weight_d_epoch_{:03d}.hdf5\".format(\n",
    "            WEIGHT_DIR, epoch), True)\n",
    "\n",
    "        # generate some digits to display\n",
    "        noise = np.random.uniform(-1, 1, (100, LATENT_SIZE))\n",
    "\n",
    "        sampled_labels = np.array([\n",
    "            [i] * 10 for i in range(10)\n",
    "        ]).reshape(-1, 1)\n",
    "\n",
    "        # get a batch to display\n",
    "        generated_images = g.predict(\n",
    "            [noise, sampled_labels], verbose=0)\n",
    "\n",
    "        # arrange them into a grid\n",
    "        img = (np.concatenate([r.reshape(-1, 28)\n",
    "                               for r in np.split(generated_images, 10)\n",
    "                               ], axis=-1) * SCALE + SCALE).astype(np.uint8)\n",
    "\n",
    "        Image.fromarray(img).save(\n",
    "            '{}plot_epoch_{:03d}_generated.png'.format(VIS_DIR, epoch))\n",
    "\n",
    "    pickle.dump({'train': train_history, 'test': test_history},\n",
    "                open('wgan-history.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 50\n",
      "\r",
      "  0/468 [..............................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chawins/.conda/envs/tsa3/lib/python3.6/site-packages/keras/engine/training.py:973: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "467/468 [============================>.] - ETA: 0s\n",
      "Testing for epoch 1:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 2.29 | 0.02            | 2.27 \n",
      "generator (test)       | 2.20 | 0.04            | 2.16 \n",
      "discriminator (train)  | 4.57 | 0.00            | 4.56 \n",
      "discriminator (test)   | 2.20 | -0.02           | 2.22 \n",
      "Epoch 2 of 50\n",
      "467/468 [============================>.] - ETA: 0s\n",
      "Testing for epoch 2:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 2.19 | 0.12            | 2.08 \n",
      "generator (test)       | 2.18 | 0.14            | 2.04 \n",
      "discriminator (train)  | 4.32 | -0.03           | 4.35 \n",
      "discriminator (test)   | 2.10 | -0.05           | 2.15 \n",
      "Epoch 3 of 50\n",
      "467/468 [============================>.] - ETA: 0s\n",
      "Testing for epoch 3:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 2.18 | 0.15            | 2.03 \n",
      "generator (test)       | 2.18 | 0.16            | 2.02 \n",
      "discriminator (train)  | 4.27 | -0.02           | 4.30 \n",
      "discriminator (test)   | 2.08 | -0.05           | 2.14 \n",
      "Epoch 4 of 50\n",
      "467/468 [============================>.] - ETA: 0s\n",
      "Testing for epoch 4:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 2.17 | 0.17            | 2.01 \n",
      "generator (test)       | 2.16 | 0.17            | 2.00 \n",
      "discriminator (train)  | 4.24 | -0.03           | 4.27 \n",
      "discriminator (test)   | 2.06 | -0.06           | 2.12 \n",
      "Epoch 5 of 50\n",
      "467/468 [============================>.] - ETA: 0s\n",
      "Testing for epoch 5:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 2.14 | 0.18            | 1.96 \n",
      "generator (test)       | 2.13 | 0.20            | 1.93 \n",
      "discriminator (train)  | 4.18 | -0.05           | 4.23 \n",
      "discriminator (test)   | 2.01 | -0.08           | 2.09 \n",
      "Epoch 6 of 50\n",
      "467/468 [============================>.] - ETA: 0s\n",
      "Testing for epoch 6:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 2.12 | 0.20            | 1.92 \n",
      "generator (test)       | 2.11 | 0.21            | 1.90 \n",
      "discriminator (train)  | 4.11 | -0.08           | 4.18 \n",
      "discriminator (test)   | 1.99 | -0.09           | 2.08 \n",
      "Epoch 7 of 50\n",
      "467/468 [============================>.] - ETA: 0s\n",
      "Testing for epoch 7:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 2.10 | 0.21            | 1.89 \n",
      "generator (test)       | 2.10 | 0.21            | 1.89 \n",
      "discriminator (train)  | 4.06 | -0.09           | 4.15 \n",
      "discriminator (test)   | 1.99 | -0.09           | 2.07 \n",
      "Epoch 8 of 50\n",
      "467/468 [============================>.] - ETA: 0s\n",
      "Testing for epoch 8:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 2.09 | 0.21            | 1.88 \n",
      "generator (test)       | 2.09 | 0.22            | 1.88 \n",
      "discriminator (train)  | 4.04 | -0.10           | 4.14 \n",
      "discriminator (test)   | 1.96 | -0.10           | 2.06 \n",
      "Epoch 9 of 50\n",
      "467/468 [============================>.] - ETA: 0s\n",
      "Testing for epoch 9:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 2.09 | 0.22            | 1.87 \n",
      "generator (test)       | 2.08 | 0.22            | 1.86 \n",
      "discriminator (train)  | 4.02 | -0.11           | 4.13 \n",
      "discriminator (test)   | 1.96 | -0.10           | 2.05 \n",
      "Epoch 10 of 50\n",
      "467/468 [============================>.] - ETA: 0s\n",
      "Testing for epoch 10:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 2.08 | 0.23            | 1.85 \n",
      "generator (test)       | 2.07 | 0.23            | 1.84 \n",
      "discriminator (train)  | 3.99 | -0.12           | 4.11 \n",
      "discriminator (test)   | 1.94 | -0.10           | 2.05 \n",
      "Epoch 11 of 50\n",
      "467/468 [============================>.] - ETA: 0s\n",
      "Testing for epoch 11:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 2.07 | 0.24            | 1.84 \n",
      "generator (test)       | 2.07 | 0.24            | 1.83 \n",
      "discriminator (train)  | 3.97 | -0.13           | 4.10 \n",
      "discriminator (test)   | 1.94 | -0.10           | 2.04 \n",
      "Epoch 12 of 50\n",
      "467/468 [============================>.] - ETA: 0s\n",
      "Testing for epoch 12:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 2.07 | 0.24            | 1.83 \n",
      "generator (test)       | 2.07 | 0.25            | 1.82 \n",
      "discriminator (train)  | 3.96 | -0.13           | 4.09 \n",
      "discriminator (test)   | 1.93 | -0.11           | 2.04 \n",
      "Epoch 13 of 50\n",
      "467/468 [============================>.] - ETA: 0s\n",
      "Testing for epoch 13:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 2.07 | 0.25            | 1.82 \n",
      "generator (test)       | 2.07 | 0.24            | 1.83 \n",
      "discriminator (train)  | 3.95 | -0.13           | 4.09 \n",
      "discriminator (test)   | 1.93 | -0.11           | 2.04 \n",
      "Epoch 14 of 50\n",
      "467/468 [============================>.] - ETA: 0s\n",
      "Testing for epoch 14:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 2.07 | 0.25            | 1.82 \n",
      "generator (test)       | 2.06 | 0.25            | 1.82 \n",
      "discriminator (train)  | 3.95 | -0.14           | 4.08 \n",
      "discriminator (test)   | 1.93 | -0.11           | 2.04 \n",
      "Epoch 15 of 50\n",
      "467/468 [============================>.] - ETA: 0s\n",
      "Testing for epoch 15:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 2.07 | 0.25            | 1.82 \n",
      "generator (test)       | 2.07 | 0.26            | 1.81 \n",
      "discriminator (train)  | 3.94 | -0.14           | 4.08 \n",
      "discriminator (test)   | 1.92 | -0.11           | 2.03 \n",
      "Epoch 16 of 50\n",
      "467/468 [============================>.] - ETA: 0s\n",
      "Testing for epoch 16:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 2.07 | 0.26            | 1.81 \n",
      "generator (test)       | 2.06 | 0.26            | 1.81 \n",
      "discriminator (train)  | 3.94 | -0.14           | 4.08 \n",
      "discriminator (test)   | 1.92 | -0.11           | 2.03 \n",
      "Epoch 17 of 50\n",
      "467/468 [============================>.] - ETA: 0s\n",
      "Testing for epoch 17:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 2.07 | 0.26            | 1.81 \n",
      "generator (test)       | 2.06 | 0.26            | 1.80 \n",
      "discriminator (train)  | 3.94 | -0.14           | 4.07 \n",
      "discriminator (test)   | 1.92 | -0.11           | 2.03 \n",
      "Epoch 18 of 50\n",
      "467/468 [============================>.] - ETA: 0s\n",
      "Testing for epoch 18:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 2.07 | 0.26            | 1.81 \n",
      "generator (test)       | 2.06 | 0.26            | 1.80 \n",
      "discriminator (train)  | 3.94 | -0.13           | 4.07 \n",
      "discriminator (test)   | 1.92 | -0.11           | 2.03 \n",
      "Epoch 19 of 50\n",
      "467/468 [============================>.] - ETA: 0s\n",
      "Testing for epoch 19:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 2.07 | 0.26            | 1.81 \n",
      "generator (test)       | 2.06 | 0.26            | 1.80 \n",
      "discriminator (train)  | 3.94 | -0.13           | 4.07 \n",
      "discriminator (test)   | 1.91 | -0.11           | 2.02 \n",
      "Epoch 20 of 50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "467/468 [============================>.] - ETA: 0s\n",
      "Testing for epoch 20:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 2.07 | 0.26            | 1.80 \n",
      "generator (test)       | 2.06 | 0.25            | 1.80 \n",
      "discriminator (train)  | 3.94 | -0.13           | 4.07 \n",
      "discriminator (test)   | 1.92 | -0.11           | 2.02 \n",
      "Epoch 21 of 50\n",
      "467/468 [============================>.] - ETA: 0s\n",
      "Testing for epoch 21:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 2.07 | 0.26            | 1.80 \n",
      "generator (test)       | 2.07 | 0.27            | 1.80 \n",
      "discriminator (train)  | 3.94 | -0.13           | 4.07 \n",
      "discriminator (test)   | 1.91 | -0.11           | 2.03 \n",
      "Epoch 22 of 50\n",
      "467/468 [============================>.] - ETA: 0s\n",
      "Testing for epoch 22:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 2.07 | 0.27            | 1.80 \n",
      "generator (test)       | 2.06 | 0.27            | 1.80 \n",
      "discriminator (train)  | 3.94 | -0.13           | 4.07 \n",
      "discriminator (test)   | 1.91 | -0.11           | 2.02 \n",
      "Epoch 23 of 50\n",
      "467/468 [============================>.] - ETA: 0s\n",
      "Testing for epoch 23:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 2.07 | 0.27            | 1.80 \n",
      "generator (test)       | 2.06 | 0.26            | 1.80 \n",
      "discriminator (train)  | 3.94 | -0.13           | 4.07 \n",
      "discriminator (test)   | 1.92 | -0.11           | 2.03 \n",
      "Epoch 24 of 50\n",
      "467/468 [============================>.] - ETA: 0s\n",
      "Testing for epoch 24:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 2.07 | 0.27            | 1.80 \n",
      "generator (test)       | 2.06 | 0.26            | 1.80 \n",
      "discriminator (train)  | 3.94 | -0.13           | 4.07 \n",
      "discriminator (test)   | 1.91 | -0.11           | 2.03 \n",
      "Epoch 25 of 50\n",
      "467/468 [============================>.] - ETA: 0s\n",
      "Testing for epoch 25:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 2.07 | 0.27            | 1.80 \n",
      "generator (test)       | 2.06 | 0.26            | 1.80 \n",
      "discriminator (train)  | 3.94 | -0.13           | 4.07 \n",
      "discriminator (test)   | 1.91 | -0.11           | 2.02 \n",
      "Epoch 26 of 50\n",
      "467/468 [============================>.] - ETA: 0s\n",
      "Testing for epoch 26:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 2.07 | 0.27            | 1.80 \n",
      "generator (test)       | 2.06 | 0.26            | 1.80 \n",
      "discriminator (train)  | 3.94 | -0.13           | 4.07 \n",
      "discriminator (test)   | 1.91 | -0.11           | 2.02 \n",
      "Epoch 27 of 50\n",
      "467/468 [============================>.] - ETA: 0s\n",
      "Testing for epoch 27:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 2.07 | 0.27            | 1.80 \n",
      "generator (test)       | 2.06 | 0.27            | 1.80 \n",
      "discriminator (train)  | 3.94 | -0.13           | 4.06 \n",
      "discriminator (test)   | 1.91 | -0.11           | 2.02 \n",
      "Epoch 28 of 50\n",
      "467/468 [============================>.] - ETA: 0s\n",
      "Testing for epoch 28:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 2.07 | 0.27            | 1.80 \n",
      "generator (test)       | 2.06 | 0.26            | 1.80 \n",
      "discriminator (train)  | 3.94 | -0.13           | 4.06 \n",
      "discriminator (test)   | 1.92 | -0.11           | 2.03 \n",
      "Epoch 29 of 50\n",
      "467/468 [============================>.] - ETA: 0s\n",
      "Testing for epoch 29:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 2.07 | 0.27            | 1.80 \n",
      "generator (test)       | 2.07 | 0.27            | 1.80 \n",
      "discriminator (train)  | 3.94 | -0.13           | 4.06 \n",
      "discriminator (test)   | 1.91 | -0.11           | 2.02 \n",
      "Epoch 30 of 50\n",
      "467/468 [============================>.] - ETA: 0s\n",
      "Testing for epoch 30:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 2.07 | 0.27            | 1.80 \n",
      "generator (test)       | 2.06 | 0.27            | 1.79 \n",
      "discriminator (train)  | 3.93 | -0.13           | 4.06 \n",
      "discriminator (test)   | 1.92 | -0.11           | 2.03 \n",
      "Epoch 31 of 50\n",
      "467/468 [============================>.] - ETA: 0s\n",
      "Testing for epoch 31:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 2.07 | 0.27            | 1.80 \n",
      "generator (test)       | 2.08 | 0.27            | 1.80 \n",
      "discriminator (train)  | 3.94 | -0.13           | 4.06 \n",
      "discriminator (test)   | 1.91 | -0.11           | 2.03 \n",
      "Epoch 32 of 50\n",
      "467/468 [============================>.] - ETA: 0s\n",
      "Testing for epoch 32:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 2.07 | 0.27            | 1.80 \n",
      "generator (test)       | 2.06 | 0.26            | 1.79 \n",
      "discriminator (train)  | 3.94 | -0.13           | 4.06 \n",
      "discriminator (test)   | 1.92 | -0.11           | 2.02 \n",
      "Epoch 33 of 50\n",
      "467/468 [============================>.] - ETA: 0s\n",
      "Testing for epoch 33:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 2.07 | 0.27            | 1.80 \n",
      "generator (test)       | 2.06 | 0.26            | 1.79 \n",
      "discriminator (train)  | 3.93 | -0.13           | 4.06 \n",
      "discriminator (test)   | 1.92 | -0.11           | 2.02 \n",
      "Epoch 34 of 50\n",
      "467/468 [============================>.] - ETA: 0s\n",
      "Testing for epoch 34:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 2.07 | 0.27            | 1.80 \n",
      "generator (test)       | 2.07 | 0.27            | 1.80 \n",
      "discriminator (train)  | 3.94 | -0.13           | 4.06 \n",
      "discriminator (test)   | 1.91 | -0.11           | 2.02 \n",
      "Epoch 35 of 50\n",
      "467/468 [============================>.] - ETA: 0s\n",
      "Testing for epoch 35:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 2.07 | 0.27            | 1.80 \n",
      "generator (test)       | 2.06 | 0.27            | 1.79 \n",
      "discriminator (train)  | 3.94 | -0.13           | 4.06 \n",
      "discriminator (test)   | 1.91 | -0.11           | 2.02 \n",
      "Epoch 36 of 50\n",
      "467/468 [============================>.] - ETA: 0s\n",
      "Testing for epoch 36:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 2.07 | 0.27            | 1.80 \n",
      "generator (test)       | 2.05 | 0.26            | 1.79 \n",
      "discriminator (train)  | 3.94 | -0.13           | 4.06 \n",
      "discriminator (test)   | 1.91 | -0.11           | 2.02 \n",
      "Epoch 37 of 50\n",
      "467/468 [============================>.] - ETA: 0s\n",
      "Testing for epoch 37:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 2.07 | 0.27            | 1.80 \n",
      "generator (test)       | 2.07 | 0.28            | 1.79 \n",
      "discriminator (train)  | 3.94 | -0.13           | 4.06 \n",
      "discriminator (test)   | 1.91 | -0.11           | 2.02 \n",
      "Epoch 38 of 50\n",
      "467/468 [============================>.] - ETA: 0s\n",
      "Testing for epoch 38:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 2.07 | 0.27            | 1.80 \n",
      "generator (test)       | 2.07 | 0.27            | 1.79 \n",
      "discriminator (train)  | 3.94 | -0.13           | 4.06 \n",
      "discriminator (test)   | 1.91 | -0.11           | 2.02 \n",
      "Epoch 39 of 50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "467/468 [============================>.] - ETA: 0s\n",
      "Testing for epoch 39:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 2.07 | 0.27            | 1.80 \n",
      "generator (test)       | 2.07 | 0.27            | 1.79 \n",
      "discriminator (train)  | 3.93 | -0.13           | 4.06 \n",
      "discriminator (test)   | 1.92 | -0.11           | 2.02 \n",
      "Epoch 40 of 50\n",
      "467/468 [============================>.] - ETA: 0s\n",
      "Testing for epoch 40:\n",
      "component              | loss | generation_loss | auxiliary_loss\n",
      "-----------------------------------------------------------------\n",
      "generator (train)      | 2.07 | 0.27            | 1.80 \n",
      "generator (test)       | 2.06 | 0.27            | 1.80 \n",
      "discriminator (train)  | 3.94 | -0.13           | 4.06 \n",
      "discriminator (test)   | 1.91 | -0.11           | 2.02 \n",
      "Epoch 41 of 50\n",
      "467/468 [============================>.] - ETA: 0s\n",
      "Testing for epoch 41:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-326b49521e8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-27d3324cf528>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(prog)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0my_aux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_sampled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0md_test_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0my_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_aux\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0md_train_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_d_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tsa3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[1;32m   1725\u001b[0m                                \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1726\u001b[0m                                \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1727\u001b[0;31m                                steps=steps)\n\u001b[0m\u001b[1;32m   1728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1729\u001b[0m     def predict(self, x,\n",
      "\u001b[0;32m~/.conda/envs/tsa3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_test_loop\u001b[0;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1368\u001b[0m                     \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tsa3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2350\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2351\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2352\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tsa3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tsa3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tsa3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tsa3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tsa3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = build_generator()\n",
    "#g.compile(optimizer=\"SGD\", loss='sparse_categorical_crossentropy')\n",
    "#g.load_weights(WEIGHT_DIR + \"weight_g_epoch_018.hdf5\")\n",
    "g.load_weights(WEIGHT_DIR + \"weight_g_epoch_005.hdf5\")\n",
    "\n",
    "d = build_discriminator()\n",
    "#d.compile(optimizer=\"SGD\", loss='sparse_categorical_crossentropy')\n",
    "#d.load_weights(WEIGHT_DIR + \"weight_d_epoch_018.hdf5\")\n",
    "d.load_weights(WEIGHT_DIR + \"weight_d_epoch_005.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_43 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 1, 100)       1000        input_43[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_42 (InputLayer)           (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_19 (Flatten)            (None, 100)          0           embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "multiply_11 (Multiply)          (None, 100)          0           input_42[0][0]                   \n",
      "                                                                 flatten_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "sequential_19 (Sequential)      (None, 28, 28, 1)    8171521     multiply_11[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 8,172,521\n",
      "Trainable params: 8,172,521\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "g.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[  3.78280804e-02,   3.90699841e-02,  -2.15779338e-03, ...,\n",
       "           6.64532045e-03,   1.80605855e-02,   6.52331561e-02],\n",
       "        [ -2.91381460e-02,   3.40126082e-02,  -1.42912909e-01, ...,\n",
       "          -6.17513582e-02,  -2.44552479e-03,  -8.97833779e-02],\n",
       "        [  1.12108253e-02,   1.29567206e-01,   1.33207515e-02, ...,\n",
       "           8.82425606e-02,   7.41734579e-02,  -6.54385658e-05],\n",
       "        ..., \n",
       "        [ -9.30008199e-03,  -6.14549667e-02,  -4.43667620e-02, ...,\n",
       "          -1.01190314e-01,   5.40581606e-02,   4.08558585e-02],\n",
       "        [ -2.87669040e-02,  -1.05905803e-02,   5.11248177e-03, ...,\n",
       "          -6.53349375e-03,  -1.54103595e-03,  -8.06531161e-02],\n",
       "        [  2.43268255e-03,  -4.13992368e-02,   1.57080172e-03, ...,\n",
       "          -3.69323231e-03,   2.90765669e-02,  -3.78117785e-02]], dtype=float32),\n",
       " array([ 0.00820533, -0.052864  ,  0.00759658, ...,  0.00253871,\n",
       "        -0.01407321,  0.00079039], dtype=float32),\n",
       " array([[ 0.05901076,  0.06434827,  0.07743584, ...,  0.06454771,\n",
       "          0.0491405 ,  0.04677062],\n",
       "        [-0.00239654, -0.0062749 ,  0.01609129, ..., -0.01804535,\n",
       "         -0.05397621,  0.01837848],\n",
       "        [-0.06518729, -0.02429355, -0.0474013 , ...,  0.0136062 ,\n",
       "          0.0222656 ,  0.02148701],\n",
       "        ..., \n",
       "        [ 0.02076559,  0.04325281,  0.01954024, ..., -0.0355514 ,\n",
       "         -0.07332197, -0.04497316],\n",
       "        [ 0.04633036,  0.03583121,  0.06700411, ..., -0.02448222,\n",
       "         -0.00579491, -0.00047232],\n",
       "        [ 0.0044028 ,  0.05376156, -0.00867828, ...,  0.02337003,\n",
       "          0.01659717,  0.08395074]], dtype=float32),\n",
       " array([-0.06978831, -0.08905524, -0.07654432, ..., -0.0231625 ,\n",
       "        -0.03668187, -0.04624686], dtype=float32),\n",
       " array([[[[  2.98288167e-02,  -2.87709367e-02,  -5.99066354e-03, ...,\n",
       "             1.90842412e-02,  -3.78978588e-02,  -3.19550484e-02],\n",
       "          [ -4.09995057e-02,  -7.44528696e-02,  -3.58773395e-02, ...,\n",
       "            -1.51209170e-02,  -3.57321575e-02,  -8.32650363e-02],\n",
       "          [  1.58914104e-02,  -1.34398835e-02,  -2.68719606e-02, ...,\n",
       "             8.09117872e-03,  -4.86569107e-03,  -2.92883329e-02],\n",
       "          ..., \n",
       "          [  5.35369776e-02,  -5.64111806e-02,  -6.00627111e-03, ...,\n",
       "            -2.47574560e-02,  -2.96144504e-02,   6.12610579e-03],\n",
       "          [  3.90778519e-02,  -5.13175912e-02,  -4.68262322e-02, ...,\n",
       "            -1.99978929e-02,  -1.93803497e-02,  -4.48540971e-03],\n",
       "          [  4.19892110e-02,  -6.15440123e-02,  -4.58772629e-02, ...,\n",
       "            -1.56349689e-02,  -2.90750582e-02,   2.41144467e-03]],\n",
       " \n",
       "         [[  4.19386551e-02,  -7.83947781e-02,  -3.41725796e-02, ...,\n",
       "            -4.47897650e-02,  -3.26340832e-02,  -1.29338674e-04],\n",
       "          [ -4.55274470e-02,  -9.75837782e-02,  -7.08093345e-02, ...,\n",
       "             1.58261955e-02,  -8.22739899e-02,  -8.60987827e-02],\n",
       "          [  2.00836454e-02,  -4.53199968e-02,  -2.77897604e-02, ...,\n",
       "            -4.15087258e-03,  -1.58338435e-02,  -4.49532457e-02],\n",
       "          ..., \n",
       "          [  2.31934991e-02,  -2.95864213e-02,  -5.44109344e-02, ...,\n",
       "            -4.86263297e-02,  -4.04171459e-02,   3.43733304e-03],\n",
       "          [  1.86264869e-02,  -2.18819119e-02,  -4.72129062e-02, ...,\n",
       "            -2.68271398e-02,  -1.91914439e-02,  -6.20069623e-04],\n",
       "          [ -3.22398706e-03,  -5.17978370e-02,  -3.68895382e-02, ...,\n",
       "            -2.00793780e-02,  -2.94628832e-02,  -2.16092542e-02]],\n",
       " \n",
       "         [[  8.32466502e-03,  -4.19872664e-02,  -5.95121831e-02, ...,\n",
       "            -8.78607389e-03,  -2.91256290e-02,  -2.27703229e-02],\n",
       "          [ -7.33815357e-02,  -9.28257108e-02,  -1.01186484e-01, ...,\n",
       "            -5.38158789e-02,  -1.02833249e-01,  -7.30473697e-02],\n",
       "          [  1.60381366e-02,  -4.06626575e-02,  -6.41310513e-02, ...,\n",
       "            -3.72285489e-03,  -2.50224061e-02,  -2.90042926e-02],\n",
       "          ..., \n",
       "          [  7.03401538e-03,  -2.17475016e-02,  -1.08866692e-02, ...,\n",
       "            -1.13408500e-02,   4.24506795e-03,  -1.83749404e-02],\n",
       "          [ -8.69071577e-03,  -2.98400633e-02,  -2.15395410e-02, ...,\n",
       "            -1.70031693e-02,   3.29034477e-02,   1.63823590e-02],\n",
       "          [ -8.35261308e-03,  -7.70920068e-02,  -5.31391874e-02, ...,\n",
       "            -3.24310623e-02,  -2.97039896e-02,   8.81520472e-03]],\n",
       " \n",
       "         [[  1.62231866e-02,  -2.50423905e-02,  -2.41575949e-03, ...,\n",
       "             2.76922658e-02,   4.85619679e-02,   1.13164624e-02],\n",
       "          [ -3.00580189e-02,  -5.48560657e-02,  -4.52513434e-02, ...,\n",
       "            -4.97357361e-02,  -4.69764657e-02,  -7.10770115e-02],\n",
       "          [  1.65026914e-03,  -2.61875428e-02,   2.50270520e-03, ...,\n",
       "            -1.77409837e-03,   2.43320689e-02,  -3.09501309e-02],\n",
       "          ..., \n",
       "          [  1.51992422e-02,  -3.21399607e-03,   2.62438320e-02, ...,\n",
       "             9.82249808e-03,   6.95150495e-02,   8.67232773e-03],\n",
       "          [ -2.48377062e-02,   4.43822853e-02,   1.54559920e-02, ...,\n",
       "            -3.10853608e-02,   3.74307930e-02,  -3.88693362e-02],\n",
       "          [ -9.58315656e-03,  -1.77004933e-02,  -3.84224439e-03, ...,\n",
       "             3.17370743e-02,   5.71722090e-02,   1.38317784e-02]],\n",
       " \n",
       "         [[ -2.45639514e-02,   7.92512372e-02,   3.37547846e-02, ...,\n",
       "            -4.48212028e-02,   3.79175358e-02,  -2.54778787e-02],\n",
       "          [ -4.08676900e-02,  -4.19138186e-03,  -7.85728171e-03, ...,\n",
       "            -6.38190284e-02,   7.36190192e-03,  -5.23956828e-02],\n",
       "          [ -5.54799568e-04,   4.63739410e-02,   4.17283773e-02, ...,\n",
       "            -4.82610762e-02,   4.29136716e-02,  -5.54124452e-02],\n",
       "          ..., \n",
       "          [ -1.65266346e-03,   1.29875258e-01,   5.32169193e-02, ...,\n",
       "            -2.91656386e-02,   9.55604389e-02,   8.89160484e-03],\n",
       "          [  4.80132364e-02,   8.38903040e-02,   7.10240379e-02, ...,\n",
       "             1.19437473e-02,   5.52331619e-02,  -5.10181859e-02],\n",
       "          [ -2.07103789e-02,   9.01144147e-02,   5.56819923e-02, ...,\n",
       "            -2.52982210e-02,   7.53626302e-02,  -6.59038592e-03]]],\n",
       " \n",
       " \n",
       "        [[[  4.85799387e-02,  -5.05101010e-02,   1.08435296e-03, ...,\n",
       "            -1.54331103e-02,  -5.18294796e-03,  -2.41736453e-02],\n",
       "          [ -5.35860620e-02,  -4.78758663e-02,  -3.92921977e-02, ...,\n",
       "            -1.78543944e-02,  -3.26775387e-02,  -8.28349441e-02],\n",
       "          [ -4.42630099e-03,  -2.83768624e-02,   1.47031406e-02, ...,\n",
       "             2.49621309e-02,  -1.25483898e-02,  -7.63342083e-02],\n",
       "          ..., \n",
       "          [  4.35335636e-02,  -2.36887913e-02,  -3.15263751e-03, ...,\n",
       "             8.43737181e-03,  -2.07153372e-02,  -1.47335473e-02],\n",
       "          [  6.58243708e-03,  -5.33904396e-02,  -5.49202450e-02, ...,\n",
       "            -1.01702884e-02,   1.87757204e-03,  -2.64949258e-02],\n",
       "          [  5.72552392e-03,  -4.50047255e-02,   7.01976800e-03, ...,\n",
       "            -1.69114694e-02,  -3.30730043e-02,  -3.32612987e-03]],\n",
       " \n",
       "         [[  2.21141130e-02,  -6.40454888e-02,  -3.69810313e-02, ...,\n",
       "            -2.92353500e-02,  -6.52730986e-02,  -1.36283608e-02],\n",
       "          [ -5.80367111e-02,  -6.70994148e-02,  -6.03341945e-02, ...,\n",
       "             2.22003330e-02,  -7.52840191e-02,  -5.85323945e-02],\n",
       "          [ -2.00116765e-02,  -7.45123625e-02,  -2.52164584e-02, ...,\n",
       "            -2.49436218e-02,  -3.65331098e-02,  -3.59719321e-02],\n",
       "          ..., \n",
       "          [  3.38890753e-03,  -3.97569239e-02,  -1.60139129e-02, ...,\n",
       "            -5.01560941e-02,  -2.23651491e-02,  -1.37877800e-02],\n",
       "          [  2.52502840e-02,  -4.58124056e-02,  -2.36024940e-03, ...,\n",
       "             2.10112962e-03,   1.50277987e-02,   9.18090250e-03],\n",
       "          [ -1.49494903e-02,  -6.44654781e-02,  -6.03667460e-02, ...,\n",
       "            -4.65076901e-02,  -5.27228341e-02,   1.12351999e-02]],\n",
       " \n",
       "         [[  1.73733174e-03,  -2.46317741e-02,  -1.70078240e-02, ...,\n",
       "            -2.26114802e-02,   1.32656982e-02,  -2.70260940e-03],\n",
       "          [ -5.01836874e-02,  -6.69439435e-02,  -1.11509331e-01, ...,\n",
       "            -1.90899391e-02,  -1.07983403e-01,  -8.21515098e-02],\n",
       "          [ -1.48329819e-02,  -6.93734884e-02,  -6.62953407e-02, ...,\n",
       "            -2.16402672e-02,  -2.95363716e-03,  -4.27242257e-02],\n",
       "          ..., \n",
       "          [  5.19591500e-04,  -1.29311355e-02,   2.28646165e-03, ...,\n",
       "             1.59217301e-03,   1.23254033e-02,   1.04958899e-02],\n",
       "          [  9.80208442e-03,  -7.17980694e-03,  -1.55145349e-03, ...,\n",
       "             3.43115069e-03,  -1.81849045e-03,  -5.60700486e-04],\n",
       "          [  1.62632223e-02,  -1.38857095e-02,   6.77350501e-04, ...,\n",
       "            -2.58101746e-02,   2.34464128e-02,  -2.12907568e-02]],\n",
       " \n",
       "         [[ -1.15831923e-02,  -1.08231464e-02,   3.63239460e-02, ...,\n",
       "             4.09704662e-04,   4.34694067e-02,   2.78976327e-03],\n",
       "          [ -4.07817625e-02,   7.71530252e-03,  -2.61444766e-02, ...,\n",
       "            -3.24750096e-02,  -1.95875894e-02,  -7.10086972e-02],\n",
       "          [ -3.16469185e-02,  -1.65174808e-02,  -2.42475662e-02, ...,\n",
       "             1.34088714e-02,   3.48399058e-02,  -4.70195115e-02],\n",
       "          ..., \n",
       "          [ -1.37234619e-02,   2.20377389e-02,   3.20915133e-02, ...,\n",
       "             1.39658048e-03,   4.84974831e-02,  -1.94051210e-02],\n",
       "          [ -5.39450087e-02,   3.71350758e-02,   2.91872006e-02, ...,\n",
       "            -6.20166808e-02,   5.26466582e-04,  -2.22689398e-02],\n",
       "          [ -4.21280675e-02,   9.73785354e-04,   3.79125401e-02, ...,\n",
       "             3.84895131e-04,   1.88768804e-02,  -7.50772748e-03]],\n",
       " \n",
       "         [[  4.05101385e-03,   9.99544635e-02,   1.01447970e-01, ...,\n",
       "            -1.05457958e-02,   8.52130353e-02,  -7.61503214e-03],\n",
       "          [ -6.86209947e-02,   7.29476064e-02,   1.42420148e-02, ...,\n",
       "            -1.03095910e-02,   1.20170945e-02,  -3.17338519e-02],\n",
       "          [ -2.29199100e-02,   8.09830427e-02,   3.72234508e-02, ...,\n",
       "            -1.86935514e-02,   4.43191826e-02,  -5.08944169e-02],\n",
       "          ..., \n",
       "          [  3.71687785e-02,   1.22245200e-01,   1.17536619e-01, ...,\n",
       "             5.13515957e-02,   1.07703120e-01,   1.37714995e-02],\n",
       "          [  4.85700220e-02,   7.89597929e-02,   7.15277344e-02, ...,\n",
       "             2.32221261e-02,   6.12179264e-02,   5.90603333e-03],\n",
       "          [  3.51851024e-02,   1.13125004e-01,   1.12579748e-01, ...,\n",
       "            -6.04519329e-04,   1.13438614e-01,  -2.86926180e-02]]],\n",
       " \n",
       " \n",
       "        [[[  1.29420524e-02,  -4.19905037e-02,  -2.97300648e-02, ...,\n",
       "            -3.01146582e-02,  -2.46847328e-02,   1.36738289e-02],\n",
       "          [ -5.17390557e-02,  -5.84285557e-02,  -2.40289699e-02, ...,\n",
       "             9.77215543e-03,  -3.23356874e-02,  -5.57376333e-02],\n",
       "          [  1.29721705e-02,  -7.79222883e-03,   7.62842689e-03, ...,\n",
       "             1.95385516e-02,  -3.14349942e-02,  -6.38816804e-02],\n",
       "          ..., \n",
       "          [  3.03115752e-02,  -2.49124728e-02,  -9.37578548e-03, ...,\n",
       "            -2.22359095e-02,  -4.68617938e-02,   7.02050142e-03],\n",
       "          [ -1.20854052e-03,  -4.36876081e-02,  -6.40165713e-03, ...,\n",
       "            -2.34845635e-02,  -2.90264767e-02,  -1.98473018e-02],\n",
       "          [  3.68977897e-02,  -3.17789763e-02,  -3.80354263e-02, ...,\n",
       "            -2.51973066e-02,  -3.04773916e-02,   1.57539826e-02]],\n",
       " \n",
       "         [[  2.67044883e-02,  -6.25619367e-02,  -5.79001829e-02, ...,\n",
       "            -6.03819527e-02,  -6.55346513e-02,   1.98001564e-02],\n",
       "          [ -4.61351648e-02,  -8.26397315e-02,  -7.07752928e-02, ...,\n",
       "             4.62061260e-03,  -8.04042816e-02,  -7.13515058e-02],\n",
       "          [ -7.98723288e-03,  -7.66045526e-02,  -6.60679713e-02, ...,\n",
       "            -1.46750938e-02,  -3.64544615e-02,  -4.67101485e-02],\n",
       "          ..., \n",
       "          [ -7.99725950e-03,  -6.32507205e-02,  -1.03971241e-02, ...,\n",
       "            -1.27954558e-02,  -3.07566468e-02,   1.67891569e-03],\n",
       "          [  3.20778526e-02,  -3.52679305e-02,   1.96855869e-02, ...,\n",
       "            -2.63648462e-02,  -4.26263316e-03,   1.36671429e-02],\n",
       "          [  1.49007579e-02,  -3.17966938e-02,  -4.10741828e-02, ...,\n",
       "            -5.88284060e-02,  -4.02900055e-02,   2.19258405e-02]],\n",
       " \n",
       "         [[  3.06960959e-02,  -1.05573060e-02,   8.45383666e-03, ...,\n",
       "            -1.17280777e-03,   8.28234886e-04,  -5.51634585e-04],\n",
       "          [ -7.37167895e-02,  -1.01341963e-01,  -1.11869842e-01, ...,\n",
       "            -3.21337841e-02,  -9.74730030e-02,  -8.46798271e-02],\n",
       "          [ -2.19859071e-02,  -3.10084205e-02,  -5.61085604e-02, ...,\n",
       "             1.60508323e-02,  -2.72681303e-02,  -2.63458118e-02],\n",
       "          ..., \n",
       "          [ -1.67766903e-02,  -1.22337202e-02,  -1.86364073e-02, ...,\n",
       "            -1.64944977e-02,   1.68719850e-02,  -3.04431245e-02],\n",
       "          [ -5.48592210e-03,  -1.02882943e-04,   6.13484392e-03, ...,\n",
       "            -5.16306646e-02,   5.51729696e-03,  -1.17192613e-02],\n",
       "          [  2.84279720e-03,  -3.54036852e-03,  -3.30579840e-02, ...,\n",
       "            -2.14984752e-02,   3.91981378e-03,  -1.24204380e-03]],\n",
       " \n",
       "         [[ -2.86175013e-02,   3.70277092e-02,   3.14847641e-02, ...,\n",
       "             1.05251148e-02,   1.84579492e-02,  -3.45866047e-02],\n",
       "          [ -7.55674690e-02,  -4.32465924e-03,  -2.89874133e-02, ...,\n",
       "            -5.51586831e-03,  -6.47761393e-03,  -5.27926572e-02],\n",
       "          [ -3.54421549e-02,  -1.77774625e-03,   2.88111717e-02, ...,\n",
       "             3.33247110e-02,   5.20733781e-02,  -3.18532884e-02],\n",
       "          ..., \n",
       "          [ -7.44790304e-06,   5.41355237e-02,   5.21403886e-02, ...,\n",
       "             2.29282454e-02,   4.96636480e-02,  -3.69103462e-03],\n",
       "          [ -3.71513702e-02,   5.62096201e-02,   3.67747322e-02, ...,\n",
       "            -4.87223230e-02,   3.13800089e-02,  -2.05330253e-02],\n",
       "          [ -1.57837253e-02,   2.68669724e-02,   6.88462034e-02, ...,\n",
       "             9.54560353e-04,   2.76419055e-02,  -2.29978152e-02]],\n",
       " \n",
       "         [[  1.97291188e-02,   1.08069740e-01,   1.13933861e-01, ...,\n",
       "             2.72891838e-02,   1.16096325e-01,  -1.37666706e-02],\n",
       "          [ -3.08402013e-02,   5.76163977e-02,   6.75819144e-02, ...,\n",
       "             7.65333744e-03,   4.16883454e-02,  -6.44171312e-02],\n",
       "          [ -1.56260878e-02,   1.25510111e-01,   8.23604986e-02, ...,\n",
       "            -6.17267983e-03,   8.86001885e-02,  -5.18667363e-02],\n",
       "          ..., \n",
       "          [  8.17075744e-02,   1.04577951e-01,   1.27950743e-01, ...,\n",
       "             6.99356347e-02,   9.75772217e-02,   5.09737525e-03],\n",
       "          [  7.13966042e-02,   8.52276757e-02,   7.90762752e-02, ...,\n",
       "             6.24167249e-02,   8.22138935e-02,  -1.67437270e-02],\n",
       "          [  6.28459379e-02,   9.73989367e-02,   1.13208689e-01, ...,\n",
       "             4.80749197e-02,   1.03446320e-01,  -1.23326471e-02]]],\n",
       " \n",
       " \n",
       "        [[[  3.56258675e-02,  -2.52932049e-02,  -9.81089380e-03, ...,\n",
       "            -8.72264523e-03,  -1.10615203e-02,   1.70552563e-02],\n",
       "          [ -1.92726161e-02,  -3.90391648e-02,  -3.31324823e-02, ...,\n",
       "            -3.28996815e-02,  -4.63399962e-02,  -4.65833694e-02],\n",
       "          [  1.35540534e-02,   6.37984462e-03,  -1.22742960e-02, ...,\n",
       "             1.36948703e-03,   5.69584128e-03,  -5.51845320e-02],\n",
       "          ..., \n",
       "          [  4.13706079e-02,   2.60356092e-03,  -2.02224888e-02, ...,\n",
       "            -7.14649865e-03,  -3.29752918e-03,   2.81745140e-02],\n",
       "          [  2.78786570e-02,  -2.22192556e-02,  -8.80380720e-03, ...,\n",
       "            -2.59196721e-02,  -6.33096381e-04,  -1.31855067e-02],\n",
       "          [  8.41747597e-03,  -3.86149585e-02,  -3.44220065e-02, ...,\n",
       "            -3.76786152e-03,  -2.79466249e-02,  -1.40937669e-02]],\n",
       " \n",
       "         [[ -1.47997600e-03,  -2.96237227e-02,  -4.34376746e-02, ...,\n",
       "            -1.82482768e-02,  -5.94662055e-02,   2.10285392e-02],\n",
       "          [ -5.34031428e-02,  -9.55650732e-02,  -6.37493283e-02, ...,\n",
       "             2.35814042e-02,  -1.01588637e-01,  -7.50973001e-02],\n",
       "          [  1.32464459e-02,  -4.34316918e-02,  -3.05398386e-02, ...,\n",
       "            -1.88663956e-02,  -2.94240806e-02,  -9.55806393e-03],\n",
       "          ..., \n",
       "          [  2.47023664e-02,  -4.96676117e-02,  -2.20451187e-02, ...,\n",
       "            -3.71405445e-02,  -8.99476558e-03,  -1.20979892e-02],\n",
       "          [ -1.08409869e-02,  -2.23948713e-02,  -9.65846051e-03, ...,\n",
       "             1.46491807e-02,   1.29959779e-02,   2.63026319e-02],\n",
       "          [  1.88848446e-03,  -5.13746291e-02,  -1.40808895e-02, ...,\n",
       "            -5.33070192e-02,  -3.22335325e-02,  -2.37707999e-02]],\n",
       " \n",
       "         [[ -1.46099199e-02,  -3.31905596e-02,  -3.08443308e-02, ...,\n",
       "             1.95274111e-02,  -2.66736094e-02,  -1.78777725e-02],\n",
       "          [ -5.87758720e-02,  -6.33459687e-02,  -8.48418847e-02, ...,\n",
       "            -4.21763062e-02,  -8.94185901e-02,  -4.31305505e-02],\n",
       "          [ -1.28455218e-02,  -3.26579660e-02,  -2.05481760e-02, ...,\n",
       "             6.09798590e-03,  -1.75895989e-02,  -4.02082317e-02],\n",
       "          ..., \n",
       "          [ -1.41723389e-02,   3.19660082e-02,   1.29262386e-02, ...,\n",
       "             9.39565152e-03,  -1.52853169e-02,   4.31832857e-03],\n",
       "          [ -3.46252248e-02,   2.46238876e-02,   2.32489426e-02, ...,\n",
       "            -2.11403891e-02,  -2.89394474e-03,  -2.36422624e-02],\n",
       "          [ -2.75016688e-02,   3.10258847e-03,  -2.13558935e-02, ...,\n",
       "            -1.54270390e-02,  -2.28807479e-02,  -3.59990187e-02]],\n",
       " \n",
       "         [[ -2.13052407e-02,   4.35487926e-02,   3.29818353e-02, ...,\n",
       "             1.17624030e-02,   3.13921198e-02,  -4.77258116e-03],\n",
       "          [ -7.50944763e-02,   5.65626249e-02,  -3.44351539e-03, ...,\n",
       "            -5.74362241e-02,  -1.25768799e-02,  -7.97831863e-02],\n",
       "          [ -8.58104043e-03,   4.29217182e-02,   4.44195271e-02, ...,\n",
       "            -2.31254753e-02,   4.67475876e-02,   4.81323135e-04],\n",
       "          ..., \n",
       "          [ -3.38416807e-02,   9.13974121e-02,   6.40418380e-02, ...,\n",
       "             1.04801878e-02,   2.34309770e-02,  -3.34317088e-02],\n",
       "          [ -8.90834071e-03,   8.75108242e-02,   7.37696514e-02, ...,\n",
       "             2.21961997e-02,   4.48648408e-02,  -2.84683462e-02],\n",
       "          [ -8.80946219e-03,   6.51513040e-02,   5.12466952e-02, ...,\n",
       "            -7.09840236e-03,   3.76436226e-02,   3.20133241e-03]],\n",
       " \n",
       "         [[  4.03717533e-02,   9.68076140e-02,   9.55227092e-02, ...,\n",
       "             8.22374001e-02,   7.97022805e-02,  -2.20137872e-02],\n",
       "          [ -2.58710328e-02,   9.81194526e-02,   1.05586246e-01, ...,\n",
       "             2.38295607e-02,   7.89493397e-02,  -7.21219331e-02],\n",
       "          [  1.13152061e-02,   1.37921274e-01,   1.32812694e-01, ...,\n",
       "             2.66169161e-02,   8.62931907e-02,  -1.19218053e-02],\n",
       "          ..., \n",
       "          [  6.66601658e-02,   1.34805381e-01,   1.22879073e-01, ...,\n",
       "             6.36274740e-02,   1.19300477e-01,  -1.58556178e-02],\n",
       "          [  6.89995661e-02,   6.73601553e-02,   8.23824257e-02, ...,\n",
       "             6.73943907e-02,   6.93698674e-02,  -5.34126349e-03],\n",
       "          [  3.74209583e-02,   1.34296671e-01,   8.32280219e-02, ...,\n",
       "             7.47992098e-02,   1.14718266e-01,  -1.51219154e-02]]],\n",
       " \n",
       " \n",
       "        [[[  2.72785202e-02,  -2.51607280e-02,   1.59902871e-02, ...,\n",
       "            -3.11443750e-02,  -2.89247707e-02,   2.44741794e-03],\n",
       "          [ -1.50409499e-02,  -5.18265739e-02,  -5.32100275e-02, ...,\n",
       "            -7.05837179e-03,  -5.84202558e-02,  -2.12408323e-02],\n",
       "          [  2.66157892e-02,   1.86582226e-02,   1.61773842e-02, ...,\n",
       "             1.01420674e-02,   6.92780036e-03,   1.25869457e-02],\n",
       "          ..., \n",
       "          [  2.60201264e-02,   5.37113938e-03,  -1.48928650e-02, ...,\n",
       "            -4.73228842e-02,  -3.24988142e-02,   1.90845318e-02],\n",
       "          [  2.87439339e-02,  -1.44293811e-02,   2.00475333e-03, ...,\n",
       "            -3.55602577e-02,  -1.13094216e-02,  -4.15213685e-03],\n",
       "          [  4.45568077e-02,   1.32335611e-02,  -1.81174316e-02, ...,\n",
       "            -4.32388149e-02,  -2.47156993e-02,  -2.55363178e-03]],\n",
       " \n",
       "         [[ -1.60013873e-03,  -1.09628560e-02,  -3.18821669e-02, ...,\n",
       "            -2.87928190e-02,  -7.58173363e-03,   2.84031872e-02],\n",
       "          [ -5.96371517e-02,  -7.06423298e-02,  -6.49470389e-02, ...,\n",
       "            -7.09547894e-03,  -8.39066431e-02,  -6.29931763e-02],\n",
       "          [  1.07334219e-02,  -5.04911281e-02,  -4.56270762e-02, ...,\n",
       "             8.35924037e-03,  -3.69575731e-02,   1.28116757e-02],\n",
       "          ..., \n",
       "          [  5.01904590e-03,  -2.78001018e-02,  -2.13683257e-03, ...,\n",
       "            -6.12666504e-03,  -1.49557246e-02,   1.46876192e-02],\n",
       "          [  1.75898187e-02,   3.95674035e-02,  -6.81322487e-03, ...,\n",
       "            -1.82380192e-02,  -1.48554994e-02,   1.44376128e-03],\n",
       "          [ -1.40765235e-02,  -5.19347936e-02,  -8.54171626e-03, ...,\n",
       "            -3.21833864e-02,  -3.83167490e-02,   2.04471368e-02]],\n",
       " \n",
       "         [[ -6.78253779e-03,   1.04542607e-02,  -1.52024860e-02, ...,\n",
       "             1.80553198e-02,   1.53662171e-02,  -6.02382980e-03],\n",
       "          [ -3.98830473e-02,  -3.75373662e-02,  -8.76865759e-02, ...,\n",
       "            -4.87846658e-02,  -7.81052113e-02,  -5.42691648e-02],\n",
       "          [ -7.20314868e-03,  -4.05466966e-02,  -4.37806137e-02, ...,\n",
       "            -1.48931015e-02,  -4.55279574e-02,  -3.89553644e-02],\n",
       "          ..., \n",
       "          [ -9.83091444e-03,   6.21799640e-02,   1.55816404e-02, ...,\n",
       "            -2.33014859e-02,   4.32990771e-03,  -8.21407512e-03],\n",
       "          [ -2.09380477e-03,   7.24228695e-02,   7.99921453e-02, ...,\n",
       "            -3.01756375e-02,   5.49072139e-02,  -2.58118343e-02],\n",
       "          [ -1.91335380e-02,   6.73428923e-03,  -1.89745408e-02, ...,\n",
       "             1.07041560e-03,   5.92238549e-03,  -1.04060732e-02]],\n",
       " \n",
       "         [[  1.90052483e-02,   7.50127733e-02,   3.75227220e-02, ...,\n",
       "            -1.79779641e-02,   5.73915876e-02,   4.64443071e-03],\n",
       "          [ -6.34504035e-02,   9.08739865e-02,   6.30702227e-02, ...,\n",
       "            -2.03061532e-02,   4.50493656e-02,  -4.71330732e-02],\n",
       "          [ -1.26569823e-03,   5.12578152e-02,   5.12466840e-02, ...,\n",
       "            -3.44206095e-02,   4.63023596e-02,  -8.72428436e-03],\n",
       "          ..., \n",
       "          [  8.49756040e-03,   9.57531333e-02,   6.12399317e-02, ...,\n",
       "             2.89255138e-02,   8.18576068e-02,  -2.52183210e-02],\n",
       "          [  3.94239910e-02,   6.47010207e-02,   5.12580089e-02, ...,\n",
       "             2.74157748e-02,   3.86801548e-02,  -3.24327387e-02],\n",
       "          [ -1.46289142e-02,   8.30302164e-02,   8.47748667e-02, ...,\n",
       "            -9.39112529e-03,   5.21797985e-02,  -2.88696866e-02]],\n",
       " \n",
       "         [[  5.02955839e-02,   9.09652039e-02,   9.37957540e-02, ...,\n",
       "             8.87033939e-02,   8.62699151e-02,  -1.06436235e-03],\n",
       "          [ -1.44317206e-02,   1.30014047e-01,   1.18298113e-01, ...,\n",
       "             4.75844778e-02,   1.05622411e-01,  -4.92848791e-02],\n",
       "          [  4.65508699e-02,   1.68687001e-01,   1.42692447e-01, ...,\n",
       "             3.56792212e-02,   1.24646209e-01,  -3.45490016e-02],\n",
       "          ..., \n",
       "          [  6.98927268e-02,   9.57682207e-02,   8.28698948e-02, ...,\n",
       "             8.05374756e-02,   1.12913430e-01,  -1.56384893e-02],\n",
       "          [  6.87424019e-02,   9.42460671e-02,   4.62578386e-02, ...,\n",
       "             6.30721748e-02,   1.00997888e-01,  -2.24555433e-02],\n",
       "          [  8.86391401e-02,   8.70824531e-02,   1.18111007e-01, ...,\n",
       "             5.10398299e-02,   1.18206054e-01,  -3.77461605e-04]]]], dtype=float32),\n",
       " array([-0.01135377,  0.02356599, -0.03310367,  0.05084863, -0.04837599,\n",
       "         0.10027736, -0.00587398,  0.03830401,  0.02141252,  0.02217007,\n",
       "        -0.07997144,  0.05227293,  0.07532628, -0.06661491,  0.09857968,\n",
       "        -0.01158547, -0.08382571,  0.01636959,  0.01772817,  0.00519869,\n",
       "        -0.07665247,  0.00722127,  0.03525788,  0.04606219, -0.06745931,\n",
       "         0.09719745, -0.01787956, -0.08520997,  0.03460624,  0.07862407,\n",
       "         0.04056174, -0.04903426, -0.05722058,  0.01663793,  0.10692381,\n",
       "        -0.06080385, -0.05261767,  0.10856762,  0.03434803, -0.10115466,\n",
       "        -0.00017973, -0.01969138,  0.0404957 , -0.06807579, -0.07066318,\n",
       "         0.04783919,  0.03102175, -0.06195287,  0.01575669, -0.02629313,\n",
       "         0.10029845, -0.04302001, -0.00634006,  0.00349448, -0.11057719,\n",
       "        -0.03502572,  0.00864297, -0.09791461,  0.0189314 , -0.02886962,\n",
       "         0.00366886, -0.06049348,  0.02858946,  0.03230191, -0.0337208 ,\n",
       "         0.03258381,  0.04048175, -0.03457579,  0.02533559, -0.03433503,\n",
       "        -0.03309728, -0.02024805, -0.0342483 ,  0.09144365,  0.04949322,\n",
       "        -0.04019707,  0.09604815, -0.07152241, -0.07422134, -0.01580896,\n",
       "        -0.0004341 ,  0.00261405, -0.10583606, -0.01105505,  0.04534722,\n",
       "         0.01172141,  0.04188646, -0.01129011, -0.06517901,  0.01059784,\n",
       "         0.03588058,  0.01211571, -0.02900323,  0.09917815,  0.01284471,\n",
       "         0.03952734,  0.04506969,  0.07525089,  0.0807595 , -0.05056402,\n",
       "        -0.06107662, -0.06708262,  0.03695056,  0.02586145,  0.02115528,\n",
       "        -0.11720245,  0.07814384, -0.0912222 , -0.09625302,  0.02902762,\n",
       "         0.01032906,  0.02068875,  0.01655138,  0.04839414,  0.01055874,\n",
       "         0.01813016, -0.07935062, -0.03201006, -0.02314952, -0.05526445,\n",
       "         0.01291732,  0.0111764 ,  0.0118852 ,  0.04035545, -0.11616632,\n",
       "         0.01516578, -0.02605011, -0.11005058,  0.07269782,  0.02863765,\n",
       "         0.01691341, -0.03381224, -0.11319929,  0.01594333,  0.01563533,\n",
       "         0.01921599,  0.00820237, -0.04364588, -0.04764622,  0.01152875,\n",
       "        -0.00627019,  0.03065858, -0.08820388, -0.03600032, -0.00327327,\n",
       "        -0.0438268 , -0.10538863,  0.01168669,  0.0080998 ,  0.01001975,\n",
       "        -0.01507351, -0.03548465, -0.01104254,  0.01572659, -0.02158551,\n",
       "        -0.02938757, -0.00130298, -0.06223574, -0.03980687,  0.06726593,\n",
       "         0.00650205,  0.00291353, -0.0751575 ,  0.02449063, -0.03163036,\n",
       "         0.01443971, -0.02542753, -0.03828143, -0.03835763, -0.00899934,\n",
       "        -0.05064922,  0.08595096,  0.01462581, -0.06339341,  0.04805006,\n",
       "         0.04393021, -0.03525442,  0.00929564, -0.0028033 ,  0.01063255,\n",
       "        -0.07110985, -0.04790848,  0.02069428, -0.03507868,  0.00435255,\n",
       "        -0.03634773,  0.06600017,  0.10031588,  0.04433684, -0.01675149,\n",
       "         0.00794699,  0.05645787, -0.03791109, -0.05912198, -0.04671514,\n",
       "        -0.006699  ,  0.0202895 , -0.03762071, -0.08889064,  0.06695665,\n",
       "        -0.02547492,  0.09872553,  0.00185916,  0.03815618,  0.00676844,\n",
       "        -0.05182309,  0.04412356,  0.02838794, -0.03828295, -0.02929545,\n",
       "         0.07136151, -0.0972835 , -0.0345956 , -0.02048841,  0.01776859,\n",
       "         0.09999695,  0.01410826,  0.01434523, -0.00075046, -0.05288039,\n",
       "         0.01281647,  0.00909856, -0.0137754 , -0.05172852,  0.0323339 ,\n",
       "         0.04904323, -0.04809327,  0.03771626, -0.0224848 ,  0.0740331 ,\n",
       "         0.04745957,  0.02239932,  0.09802443,  0.04475494,  0.03845137,\n",
       "        -0.00658395,  0.04638578,  0.00705476, -0.08276335, -0.02131327,\n",
       "        -0.02341834,  0.03345847,  0.09256696,  0.03146672,  0.05023935,\n",
       "        -0.1021295 ,  0.00659168,  0.01258964,  0.09404654, -0.08946531,\n",
       "         0.01674322, -0.00837604,  0.01482296, -0.0485712 , -0.02887121,\n",
       "         0.00780783], dtype=float32),\n",
       " array([[[[ -2.79380344e-02,   5.45251928e-03,   3.04183699e-02, ...,\n",
       "            -8.37552454e-03,  -5.09948283e-02,   2.31916290e-02],\n",
       "          [ -8.14636126e-02,  -1.66703817e-02,   2.18840931e-02, ...,\n",
       "             2.16840226e-02,  -9.88350958e-02,   2.54309122e-02],\n",
       "          [ -1.33524641e-01,   1.63146723e-02,   2.91766301e-02, ...,\n",
       "             3.26783545e-02,  -1.46226630e-01,   5.45324758e-02],\n",
       "          ..., \n",
       "          [ -8.88637677e-02,   7.82860219e-02,  -1.09164941e-03, ...,\n",
       "             4.37980630e-02,  -1.19488418e-01,   1.34262182e-02],\n",
       "          [ -1.18691534e-01,   1.08937221e-02,   3.74299251e-02, ...,\n",
       "             2.10636482e-02,  -9.53019112e-02,   7.74847120e-02],\n",
       "          [  3.94991450e-02,  -1.48228547e-02,   1.72140859e-02, ...,\n",
       "            -9.77510288e-02,   4.40942794e-02,  -6.42695278e-02]],\n",
       " \n",
       "         [[ -1.34477287e-03,   3.97662586e-03,   7.20160529e-02, ...,\n",
       "             8.31792876e-03,  -3.33291553e-02,  -1.09977480e-02],\n",
       "          [ -7.93846771e-02,   4.51548100e-02,   1.34963214e-01, ...,\n",
       "             4.98458222e-02,  -1.26708865e-01,  -9.33340844e-03],\n",
       "          [ -8.32181126e-02,   6.59528896e-02,   1.07689202e-01, ...,\n",
       "             6.00329265e-02,  -1.50604919e-01,  -2.62391614e-03],\n",
       "          ..., \n",
       "          [  4.64839395e-04,   8.13476518e-02,   7.98197463e-02, ...,\n",
       "             5.99819869e-02,  -9.78733078e-02,  -5.01073748e-02],\n",
       "          [ -6.31447807e-02,   4.23041172e-02,   1.02040239e-01, ...,\n",
       "             7.26757050e-02,  -1.52819976e-01,  -1.63480267e-02],\n",
       "          [  1.24040833e-02,  -3.58723034e-03,   5.63278571e-02, ...,\n",
       "            -5.27149066e-02,   2.58266050e-02,  -8.04581791e-02]],\n",
       " \n",
       "         [[  8.01693462e-03,   3.24875535e-03,  -1.23504817e-03, ...,\n",
       "            -3.29644792e-02,   1.25659897e-03,  -2.20321100e-02],\n",
       "          [ -8.16886574e-02,   5.63515611e-02,   1.15749277e-01, ...,\n",
       "             2.16245912e-02,  -8.51675794e-02,  -5.30085452e-02],\n",
       "          [ -6.73146173e-02,   1.02645665e-01,   7.87524655e-02, ...,\n",
       "             1.32682510e-02,  -7.51277134e-02,  -2.89757475e-02],\n",
       "          ..., \n",
       "          [ -6.87983772e-03,   3.89355980e-02,   5.24907298e-02, ...,\n",
       "            -3.85317728e-02,  -4.64226939e-02,  -3.73411886e-02],\n",
       "          [ -5.55056296e-02,   5.52476496e-02,   5.18214330e-02, ...,\n",
       "             5.96484765e-02,  -8.78340602e-02,   1.06960824e-02],\n",
       "          [  3.33694629e-02,   9.45513789e-03,  -1.02496482e-02, ...,\n",
       "            -5.68461791e-02,   1.47166830e-02,  -3.96895297e-02]],\n",
       " \n",
       "         [[ -1.24828350e-02,   2.15399433e-02,   5.29401284e-03, ...,\n",
       "            -5.41236764e-03,  -1.92878451e-02,  -2.14964822e-02],\n",
       "          [  2.87528671e-02,  -1.71167322e-03,   3.31999101e-02, ...,\n",
       "            -1.32305385e-03,  -5.96648231e-02,  -1.06899619e-01],\n",
       "          [  2.20545866e-02,   1.18144965e-02,   5.15694693e-02, ...,\n",
       "             8.31867848e-03,  -2.84393262e-02,  -9.21374187e-02],\n",
       "          ..., \n",
       "          [ -2.32632272e-02,  -1.26120066e-02,  -3.89570300e-03, ...,\n",
       "            -4.05579619e-02,  -5.94914816e-02,  -6.24974295e-02],\n",
       "          [  3.15527245e-02,   2.68733110e-02,   5.92921153e-02, ...,\n",
       "             1.55022899e-02,  -8.65523815e-02,  -9.21748579e-02],\n",
       "          [  1.39696859e-02,   3.38929594e-02,   1.13775581e-02, ...,\n",
       "            -5.81750907e-02,  -1.72382873e-02,  -6.92964420e-02]],\n",
       " \n",
       "         [[ -1.91032849e-02,   5.39579205e-02,  -8.67106859e-03, ...,\n",
       "            -4.22100015e-02,  -1.60652734e-02,  -1.31886965e-02],\n",
       "          [  1.49242636e-02,  -4.27677371e-02,   7.10785836e-02, ...,\n",
       "            -7.53478557e-02,   1.93803143e-02,  -9.27251801e-02],\n",
       "          [  9.55561642e-03,  -8.35529435e-03,   2.64874194e-02, ...,\n",
       "            -6.23408146e-02,   3.23142558e-02,  -2.11422406e-02],\n",
       "          ..., \n",
       "          [ -1.31289866e-02,  -2.15100367e-02,  -4.06222083e-02, ...,\n",
       "            -3.96509655e-02,   4.40340111e-04,   3.13547999e-03],\n",
       "          [  1.14574339e-02,  -8.81859846e-03,   1.41366189e-02, ...,\n",
       "            -4.22983654e-02,   3.57026532e-02,  -3.91172394e-02],\n",
       "          [  2.63125692e-02,  -3.16787846e-02,  -1.40668359e-02, ...,\n",
       "            -6.78477585e-02,  -6.98192464e-03,  -5.94593063e-02]]],\n",
       " \n",
       " \n",
       "        [[[ -1.10416096e-02,  -2.50720233e-02,   7.52054825e-02, ...,\n",
       "             1.74878705e-02,  -7.41336197e-02,   1.41722485e-02],\n",
       "          [ -1.27734646e-01,   3.43942083e-02,   2.33679470e-02, ...,\n",
       "             4.33003064e-03,  -9.30264220e-02,   3.46179567e-02],\n",
       "          [ -1.30470946e-01,   4.04613987e-02,   2.78029796e-02, ...,\n",
       "             4.39885929e-02,  -1.28012106e-01,   6.08022772e-02],\n",
       "          ..., \n",
       "          [ -8.36820900e-02,   6.74797893e-02,   2.40701940e-02, ...,\n",
       "             5.17770164e-02,  -9.18725058e-02,   4.25066948e-02],\n",
       "          [ -1.10942282e-01,   2.34586801e-02,   3.89409624e-02, ...,\n",
       "             2.82170586e-02,  -1.22699611e-01,   8.74172747e-02],\n",
       "          [  5.18594012e-02,   2.89360117e-02,   1.74271688e-02, ...,\n",
       "            -9.14819911e-02,   1.66156441e-02,  -4.18213680e-02]],\n",
       " \n",
       "         [[ -2.28352938e-02,   1.40378727e-02,   7.64391422e-02, ...,\n",
       "             2.74508614e-02,  -4.72537912e-02,  -2.29273103e-02],\n",
       "          [ -1.20506756e-01,   6.27506003e-02,   1.09373063e-01, ...,\n",
       "             6.62911683e-02,  -1.20019503e-01,  -1.65046528e-02],\n",
       "          [ -7.05427006e-02,   7.41602927e-02,   9.38969254e-02, ...,\n",
       "             5.16280048e-02,  -1.35722518e-01,  -3.03440653e-02],\n",
       "          ..., \n",
       "          [ -4.80045378e-03,   7.21349493e-02,   1.12607531e-01, ...,\n",
       "             6.52579516e-02,  -7.95460194e-02,  -3.37161832e-02],\n",
       "          [ -7.61874691e-02,   6.47523701e-02,   5.79423681e-02, ...,\n",
       "             7.83820823e-02,  -1.24243118e-01,  -2.75170412e-02],\n",
       "          [  3.93511541e-02,   4.72613350e-02,   4.49830480e-02, ...,\n",
       "            -6.90931380e-02,  -3.42084002e-03,  -5.43801710e-02]],\n",
       " \n",
       "         [[ -1.20025249e-02,   3.28284577e-02,   3.30375284e-02, ...,\n",
       "             4.77391761e-03,  -2.28763069e-03,  -3.29309590e-02],\n",
       "          [ -3.88974771e-02,   5.78373373e-02,   1.00151628e-01, ...,\n",
       "            -9.99123976e-03,  -1.12617873e-01,  -3.49313542e-02],\n",
       "          [ -6.93314569e-03,   4.15996127e-02,   6.48733228e-02, ...,\n",
       "            -4.45394404e-02,  -7.61792287e-02,  -2.86369678e-02],\n",
       "          ..., \n",
       "          [ -2.77753677e-02,   2.26010382e-02,   4.68625426e-02, ...,\n",
       "            -3.29845473e-02,  -2.45889481e-02,  -1.14322891e-02],\n",
       "          [ -9.13282670e-03,   5.77831753e-02,   7.81067684e-02, ...,\n",
       "            -1.71483587e-02,  -8.67957249e-02,   5.01600001e-03],\n",
       "          [  9.38955881e-03,   1.83927473e-02,  -2.01961230e-02, ...,\n",
       "            -5.57247289e-02,   8.36961786e-04,  -3.43736187e-02]],\n",
       " \n",
       "         [[ -3.48245259e-04,   3.33163887e-02,  -2.11036913e-02, ...,\n",
       "            -2.00036447e-02,  -9.59295221e-03,  -2.39092186e-02],\n",
       "          [  4.54563601e-03,   4.48895060e-02,   1.92117766e-02, ...,\n",
       "             1.67975556e-02,  -5.23339361e-02,  -7.68210962e-02],\n",
       "          [  4.59961556e-02,   3.42604779e-02,   1.05393608e-03, ...,\n",
       "             1.08017037e-02,  -3.89602743e-02,  -8.33534747e-02],\n",
       "          ..., \n",
       "          [ -1.86907109e-02,  -1.77807156e-02,   3.57384570e-02, ...,\n",
       "            -4.52344492e-02,  -2.01656520e-02,  -1.23966094e-02],\n",
       "          [  1.07820192e-02,   5.57392985e-02,   4.14299555e-02, ...,\n",
       "             1.91826618e-03,  -2.96817217e-02,  -7.60245472e-02],\n",
       "          [  1.99402012e-02,   4.14950661e-02,   3.01772496e-04, ...,\n",
       "            -2.45464984e-02,  -7.80600682e-03,  -4.18869928e-02]],\n",
       " \n",
       "         [[ -2.63985060e-02,   3.66264880e-02,  -1.39893694e-02, ...,\n",
       "            -2.85454448e-02,  -1.65068321e-02,  -5.56862764e-02],\n",
       "          [  6.73839869e-03,   8.10915604e-03,  -3.82765336e-03, ...,\n",
       "            -1.06152698e-01,   1.97203290e-02,  -3.58041599e-02],\n",
       "          [ -9.12669487e-03,   1.74521226e-02,  -2.46356651e-02, ...,\n",
       "            -4.61721346e-02,   1.48475366e-02,  -1.62610281e-02],\n",
       "          ..., \n",
       "          [ -5.32039329e-02,   1.63959339e-02,  -6.61293343e-02, ...,\n",
       "            -5.04687391e-02,  -4.06667255e-02,  -4.38305773e-02],\n",
       "          [  9.72146727e-03,   3.70079651e-02,   1.83871351e-02, ...,\n",
       "            -9.43615139e-02,   3.76530886e-02,  -2.37726178e-02],\n",
       "          [ -7.69611401e-03,  -2.32346868e-03,   7.87284132e-03, ...,\n",
       "            -6.51990771e-02,   2.49766614e-02,  -4.79042679e-02]]],\n",
       " \n",
       " \n",
       "        [[[ -1.71235446e-02,  -1.90823972e-02,   3.23109841e-03, ...,\n",
       "             1.00770565e-02,  -4.67648841e-02,  -2.67709270e-02],\n",
       "          [ -1.15489572e-01,   3.58326770e-02,   5.06799817e-02, ...,\n",
       "             2.51085460e-02,  -1.32144868e-01,   4.72511537e-02],\n",
       "          [ -9.39787105e-02,   1.00871369e-01,   4.73851431e-03, ...,\n",
       "             4.65573221e-02,  -1.20846979e-01,   8.93798191e-03],\n",
       "          ..., \n",
       "          [ -4.76597920e-02,   1.26408294e-01,  -3.32476059e-03, ...,\n",
       "             5.94485067e-02,  -8.56858194e-02,  -5.28554525e-03],\n",
       "          [ -7.99707621e-02,   5.78331873e-02,  -4.08248091e-03, ...,\n",
       "             5.51846288e-02,  -8.35276246e-02,   5.17200353e-03],\n",
       "          [  3.44947800e-02,  -2.22621150e-02,   4.45377640e-02, ...,\n",
       "            -4.52115908e-02,   9.21656098e-03,  -4.53810468e-02]],\n",
       " \n",
       "         [[ -3.71268764e-02,   3.72143723e-02,   5.34745306e-02, ...,\n",
       "            -8.45661014e-03,  -6.09615743e-02,  -1.73250288e-02],\n",
       "          [ -1.06717750e-01,   5.33587262e-02,   1.00559674e-01, ...,\n",
       "             5.60914800e-02,  -1.35500029e-01,  -3.31564806e-02],\n",
       "          [ -7.05501959e-02,   1.03621840e-01,   7.72802457e-02, ...,\n",
       "             6.00600168e-02,  -7.53977671e-02,  -1.14773475e-02],\n",
       "          ..., \n",
       "          [ -2.63849404e-02,   8.60564336e-02,   9.34903920e-02, ...,\n",
       "             4.30567637e-02,  -6.80013299e-02,  -6.93198591e-02],\n",
       "          [ -7.29746893e-02,   8.08260739e-02,   8.02755281e-02, ...,\n",
       "             5.96560948e-02,  -1.07048891e-01,  -1.59283616e-02],\n",
       "          [  1.88399069e-02,   1.23019814e-02,   2.23028101e-02, ...,\n",
       "            -3.58461030e-02,   1.72029957e-02,  -4.96941991e-02]],\n",
       " \n",
       "         [[  2.53318790e-02,   3.17782909e-02,   2.56486190e-03, ...,\n",
       "            -3.03033367e-02,  -3.03349481e-03,  -3.68134789e-02],\n",
       "          [ -4.43077683e-02,   6.01487979e-02,   8.06341395e-02, ...,\n",
       "             9.83038545e-03,  -6.70296997e-02,  -3.99244614e-02],\n",
       "          [ -6.03967067e-03,   5.36327250e-02,   8.82239789e-02, ...,\n",
       "            -4.09869663e-02,  -3.47687267e-02,  -3.14930379e-02],\n",
       "          ..., \n",
       "          [ -2.11204682e-02,  -1.34607749e-02,   3.56571712e-02, ...,\n",
       "            -5.93788549e-02,  -3.04578692e-02,  -1.09383175e-02],\n",
       "          [ -2.40256377e-02,   6.21863119e-02,   9.63542312e-02, ...,\n",
       "             7.76091125e-03,  -6.59913570e-02,  -2.67689992e-02],\n",
       "          [  4.27367762e-02,  -1.73022095e-02,  -3.72290285e-03, ...,\n",
       "            -6.82363287e-02,  -9.64033511e-03,  -2.66951323e-02]],\n",
       " \n",
       "         [[ -1.99602600e-02,   2.52754521e-02,  -3.33991833e-02, ...,\n",
       "            -9.31971427e-03,  -1.19664073e-02,  -2.50332188e-02],\n",
       "          [  1.49164582e-02,  -1.43707739e-02,   5.26876524e-02, ...,\n",
       "            -2.41377763e-02,  -4.60425913e-02,  -9.76663008e-02],\n",
       "          [ -7.70384632e-03,   3.27672958e-02,   4.52834219e-02, ...,\n",
       "            -3.46830883e-03,  -2.37009637e-02,  -7.15730786e-02],\n",
       "          ..., \n",
       "          [ -2.00924780e-02,  -2.76246928e-02,   1.10114561e-02, ...,\n",
       "            -5.37731200e-02,  -5.20089008e-02,  -4.25397716e-02],\n",
       "          [  1.67835969e-02,   2.60683186e-02,   3.10498625e-02, ...,\n",
       "             2.99207005e-03,  -1.60305407e-02,  -9.07614976e-02],\n",
       "          [  5.59164537e-03,   6.07983302e-03,   8.37043393e-03, ...,\n",
       "            -5.31126931e-02,   2.01240815e-02,  -7.71165192e-02]],\n",
       " \n",
       "         [[ -1.40223419e-02,   5.39838374e-02,  -1.90286413e-02, ...,\n",
       "            -3.97984460e-02,  -3.12686786e-02,  -1.66861117e-02],\n",
       "          [  7.64738396e-03,  -3.94493230e-02,   3.29715200e-03, ...,\n",
       "            -5.70574515e-02,  -2.23182351e-03,  -5.88590205e-02],\n",
       "          [  1.48386229e-02,  -2.02154787e-03,  -2.55444217e-02, ...,\n",
       "            -4.48328927e-02,   2.41289916e-03,  -4.97017615e-02],\n",
       "          ..., \n",
       "          [ -1.13820322e-02,  -4.84819859e-02,  -2.56436300e-02, ...,\n",
       "            -5.48975356e-02,  -4.90536131e-02,  -1.80831570e-02],\n",
       "          [  2.72010658e-02,  -1.51103893e-02,   3.15400846e-02, ...,\n",
       "            -4.08499800e-02,   2.94006392e-02,  -2.59395745e-02],\n",
       "          [  7.30287284e-03,  -3.35024372e-02,  -4.12762072e-03, ...,\n",
       "            -5.24536557e-02,   4.11821604e-02,  -7.83240125e-02]]],\n",
       " \n",
       " \n",
       "        [[[ -5.08165024e-02,  -2.85619721e-02,   3.77818942e-02, ...,\n",
       "             3.47923487e-03,  -7.58072063e-02,  -1.30540803e-02],\n",
       "          [ -9.37989131e-02,   5.57396971e-02,   3.31467502e-02, ...,\n",
       "             3.56384441e-02,  -1.15785792e-01,   4.83775474e-02],\n",
       "          [ -6.91289604e-02,   9.18795019e-02,   2.29098033e-02, ...,\n",
       "             4.21602577e-02,  -7.69928992e-02,   1.47961397e-02],\n",
       "          ..., \n",
       "          [ -1.34973573e-02,   5.18664829e-02,   4.10249792e-02, ...,\n",
       "             2.65820697e-02,  -5.50630540e-02,  -7.79144373e-03],\n",
       "          [ -1.05480589e-01,   1.05311230e-01,   3.62365805e-02, ...,\n",
       "             4.26051132e-02,  -9.64973122e-02,   7.87022151e-03],\n",
       "          [  1.61757935e-02,  -1.34822726e-02,   6.44455804e-03, ...,\n",
       "            -5.40353693e-02,   1.05074299e-02,  -5.91323487e-02]],\n",
       " \n",
       "         [[ -2.12619510e-02,  -1.77463144e-02,   6.20139763e-02, ...,\n",
       "             2.59147398e-02,  -6.07063547e-02,  -4.45011333e-02],\n",
       "          [ -6.91284463e-02,   3.83054949e-02,   1.29041746e-01, ...,\n",
       "             4.09797430e-02,  -1.44462198e-01,  -2.41000410e-02],\n",
       "          [ -5.87188825e-02,   4.73431647e-02,   6.30881339e-02, ...,\n",
       "             2.63332203e-02,  -6.66868538e-02,  -1.69314165e-02],\n",
       "          ..., \n",
       "          [ -9.46417172e-03,   2.71967892e-02,   4.58556451e-02, ...,\n",
       "             2.92086881e-02,  -5.71960062e-02,  -7.19035938e-02],\n",
       "          [ -3.42442319e-02,   7.11929798e-02,   7.95431286e-02, ...,\n",
       "             4.47272770e-02,  -7.42004886e-02,  -4.45083715e-02],\n",
       "          [ -5.80554502e-03,   2.73909178e-02,   1.15421321e-03, ...,\n",
       "            -1.58425625e-02,  -1.13985995e-02,  -3.79378460e-02]],\n",
       " \n",
       "         [[  2.35750619e-03,   2.86094230e-02,   2.47637238e-02, ...,\n",
       "             2.40592360e-02,   7.14838784e-03,  -1.18799703e-02],\n",
       "          [ -1.94523931e-02,   1.46037191e-02,   7.27793947e-02, ...,\n",
       "            -2.28076223e-02,  -4.00688089e-02,  -5.05664796e-02],\n",
       "          [ -3.40609066e-03,   9.56056546e-03,   5.73041104e-02, ...,\n",
       "            -3.24702971e-02,  -3.92728634e-02,  -3.25634815e-02],\n",
       "          ..., \n",
       "          [ -2.15764809e-02,  -4.45933500e-03,   2.95865387e-02, ...,\n",
       "            -3.92552316e-02,  -5.38548194e-02,  -1.79907884e-02],\n",
       "          [ -7.09210802e-03,   3.52585055e-02,   5.50777391e-02, ...,\n",
       "            -7.23420782e-03,  -4.54305634e-02,  -4.89358455e-02],\n",
       "          [  2.52422746e-02,  -2.73321308e-02,  -1.99189838e-02, ...,\n",
       "            -1.49086611e-02,  -1.04082376e-02,  -4.33913805e-02]],\n",
       " \n",
       "         [[ -4.09354968e-03,  -2.15936005e-02,  -2.52394602e-02, ...,\n",
       "             1.97924804e-02,   3.36334365e-03,  -3.37284617e-02],\n",
       "          [  2.39884537e-02,  -1.28162559e-02,   6.20884309e-03, ...,\n",
       "             8.08911677e-03,  -3.12506855e-02,  -7.99449310e-02],\n",
       "          [  2.49238499e-02,   2.39433236e-02,  -2.76242639e-03, ...,\n",
       "            -4.67636343e-03,  -2.35079397e-02,  -5.18111400e-02],\n",
       "          ..., \n",
       "          [ -1.22739403e-02,   1.86320152e-02,  -2.52038185e-02, ...,\n",
       "             2.85576447e-03,  -2.68378053e-02,  -1.17298355e-02],\n",
       "          [ -1.60916671e-02,   3.65596265e-02,   2.05174573e-02, ...,\n",
       "             5.55757014e-03,  -2.14574970e-02,  -6.49530366e-02],\n",
       "          [  2.09235270e-02,  -1.73839685e-02,   3.10407598e-02, ...,\n",
       "            -3.08166854e-02,   1.60556249e-02,  -6.50125146e-02]],\n",
       " \n",
       "         [[ -1.31869745e-02,  -1.30222477e-02,   1.11648692e-02, ...,\n",
       "            -1.00302119e-02,  -2.51373313e-02,  -2.27732696e-02],\n",
       "          [  2.29596365e-02,  -1.05772512e-02,  -2.58455779e-02, ...,\n",
       "            -8.30481350e-02,   3.29689197e-02,  -8.35920200e-02],\n",
       "          [  2.21446175e-02,   7.67176785e-03,  -3.61691229e-02, ...,\n",
       "            -4.65083383e-02,   2.74187513e-02,  -4.06912826e-02],\n",
       "          ..., \n",
       "          [ -2.85265502e-02,   3.55617306e-03,  -8.69465917e-02, ...,\n",
       "            -1.97118856e-02,  -1.25811752e-02,  -1.25209829e-02],\n",
       "          [  2.29608770e-02,  -9.71358269e-04,  -3.87878716e-02, ...,\n",
       "            -4.43236306e-02,   1.91314723e-02,  -2.84663811e-02],\n",
       "          [  2.39041336e-02,  -7.18399929e-03,  -3.19191776e-02, ...,\n",
       "            -3.96507643e-02,  -2.58738617e-03,  -5.61852232e-02]]],\n",
       " \n",
       " \n",
       "        [[[ -5.55520579e-02,   7.46835768e-03,   3.71059850e-02, ...,\n",
       "             2.37674657e-02,  -6.24979883e-02,  -6.47719903e-03],\n",
       "          [ -9.38294753e-02,   3.95058207e-02,   6.26838356e-02, ...,\n",
       "             4.75740395e-02,  -1.35184184e-01,   4.87805251e-03],\n",
       "          [ -5.44978157e-02,   7.79774487e-02,   6.37044683e-02, ...,\n",
       "             7.50746801e-02,  -8.78385305e-02,  -3.27721387e-02],\n",
       "          ..., \n",
       "          [ -3.70783769e-02,   1.01740792e-01,   3.63620482e-02, ...,\n",
       "             4.93022464e-02,  -4.56118882e-02,   1.70199643e-03],\n",
       "          [ -7.52280056e-02,   7.44071081e-02,   9.67704505e-03, ...,\n",
       "             9.43833590e-02,  -7.71878138e-02,  -2.78080814e-02],\n",
       "          [  1.32943541e-02,  -4.63436022e-02,   1.70105435e-02, ...,\n",
       "            -3.69151607e-02,   1.94047112e-02,  -2.76396517e-02]],\n",
       " \n",
       "         [[ -4.30971645e-02,   4.31524869e-03,   4.13743742e-02, ...,\n",
       "            -6.85975712e-04,  -4.36520427e-02,   8.33381957e-04],\n",
       "          [ -6.87905103e-02,   8.87596142e-03,   1.21824242e-01, ...,\n",
       "             3.46924849e-02,  -1.41221538e-01,  -4.39399295e-02],\n",
       "          [ -4.58631888e-02,   2.73697022e-02,   9.71888900e-02, ...,\n",
       "             5.35661913e-02,  -6.26585260e-02,  -4.98376861e-02],\n",
       "          ..., \n",
       "          [ -5.33646718e-02,   2.89466474e-02,   1.93814170e-02, ...,\n",
       "             2.97310879e-03,  -4.62684780e-02,  -5.42427525e-02],\n",
       "          [ -1.15710702e-02,   5.28724454e-02,   7.75614977e-02, ...,\n",
       "             6.24556653e-02,  -7.89003447e-02,  -4.70850430e-02],\n",
       "          [ -2.71425350e-03,  -1.03203375e-02,   3.98601890e-02, ...,\n",
       "            -1.62920672e-02,   4.25737305e-03,  -5.56599312e-02]],\n",
       " \n",
       "         [[ -3.88469809e-04,  -4.59168060e-03,   1.97244491e-02, ...,\n",
       "            -2.72909440e-02,  -9.13047884e-03,   6.05359732e-04],\n",
       "          [ -9.92935710e-03,  -5.54184057e-02,   8.21765587e-02, ...,\n",
       "            -6.56266650e-03,  -6.07926771e-02,  -4.68735360e-02],\n",
       "          [ -4.44248726e-04,  -3.50064076e-02,   2.44607236e-02, ...,\n",
       "            -1.70734581e-02,  -6.03420064e-02,  -5.92718422e-02],\n",
       "          ..., \n",
       "          [ -2.37501897e-02,  -3.51954699e-02,   4.53115022e-03, ...,\n",
       "            -1.67715382e-02,  -3.54300104e-02,  -1.60103645e-02],\n",
       "          [ -2.16240790e-02,  -5.47442548e-02,   2.63433065e-02, ...,\n",
       "            -3.71195525e-02,  -6.97513670e-02,  -4.96932901e-02],\n",
       "          [  1.70529746e-02,  -5.57622351e-02,  -2.12908629e-02, ...,\n",
       "            -3.24061401e-02,   9.08710994e-03,  -5.31558208e-02]],\n",
       " \n",
       "         [[ -3.26128751e-02,   1.54275345e-02,  -2.78423149e-02, ...,\n",
       "            -2.30369102e-02,   2.22942065e-02,  -3.81789804e-02],\n",
       "          [ -1.24341354e-03,  -2.23126989e-02,   5.07373922e-02, ...,\n",
       "            -1.65328421e-02,  -3.22324112e-02,  -6.65604621e-02],\n",
       "          [ -4.13344335e-03,   1.86984278e-02,   2.11267173e-02, ...,\n",
       "            -3.07856482e-02,  -1.36152274e-04,  -7.21814856e-02],\n",
       "          ..., \n",
       "          [ -4.67986725e-02,  -1.29504818e-02,  -8.62849317e-03, ...,\n",
       "            -3.77415903e-02,  -2.37941016e-02,  -1.50655825e-02],\n",
       "          [  8.32848996e-03,   2.20159162e-03,   2.56877355e-02, ...,\n",
       "            -1.25856670e-02,  -3.64891402e-02,  -6.80919662e-02],\n",
       "          [  3.49215567e-02,  -2.82504335e-02,   1.51691204e-02, ...,\n",
       "            -2.95558758e-02,   3.30797024e-03,  -6.81340098e-02]],\n",
       " \n",
       "         [[ -2.08037980e-02,   2.54230089e-02,  -4.29088511e-02, ...,\n",
       "            -4.07818891e-03,   1.16257854e-02,  -3.32122780e-02],\n",
       "          [  1.81293469e-02,  -7.97942653e-02,   1.48853948e-02, ...,\n",
       "            -8.50866660e-02,   3.72117274e-02,  -4.03845459e-02],\n",
       "          [  2.04925444e-02,  -1.62179973e-02,  -2.05435436e-02, ...,\n",
       "            -6.14617690e-02,   2.77258195e-02,  -2.67190002e-02],\n",
       "          ..., \n",
       "          [ -1.11562721e-02,  -2.43759342e-02,  -7.77419284e-02, ...,\n",
       "            -2.80785821e-02,  -3.65824923e-02,   1.01110125e-02],\n",
       "          [  1.01323305e-02,  -9.43794567e-03,   6.99588470e-03, ...,\n",
       "            -2.79198978e-02,  -5.24024572e-03,  -3.49802338e-02],\n",
       "          [  3.80307846e-02,  -4.26448323e-02,   1.73888467e-02, ...,\n",
       "            -5.56221940e-02,   4.02265508e-03,  -6.75308332e-02]]]], dtype=float32),\n",
       " array([ 0.08975196,  0.00790948,  0.11266792, -0.0609328 , -0.03198337,\n",
       "         0.02774424,  0.07717229,  0.06605459,  0.1083602 ,  0.05413202,\n",
       "         0.11190866, -0.07718118, -0.03730527, -0.03235789, -0.06549264,\n",
       "        -0.00787046,  0.09355281,  0.11301938,  0.08605094,  0.06332985,\n",
       "        -0.06524456, -0.0013505 , -0.00680356,  0.09697894,  0.09548768,\n",
       "         0.06254338,  0.0928001 , -0.06510617,  0.09352865, -0.01575613,\n",
       "        -0.11775128,  0.10645286, -0.04231459,  0.1213425 ,  0.08735757,\n",
       "        -0.07099015,  0.10981297,  0.08464832,  0.09271201,  0.03524799,\n",
       "         0.06445155, -0.07699385, -0.09974945,  0.03799651,  0.09257531,\n",
       "         0.09205772, -0.06572696,  0.0009206 ,  0.08407365,  0.11995342,\n",
       "        -0.00903124, -0.00535164,  0.00484567,  0.09433669, -0.11388107,\n",
       "         0.01420797,  0.11955084,  0.09274409,  0.02133609,  0.07781836,\n",
       "         0.07924002,  0.1156918 , -0.02767485,  0.09995463, -0.07328447,\n",
       "         0.10312418,  0.11180043,  0.11373583,  0.06352696,  0.09423427,\n",
       "        -0.02131245,  0.03412056,  0.08504283,  0.02411997, -0.08658222,\n",
       "         0.0935511 ,  0.00263334,  0.10328409,  0.10603293,  0.111633  ,\n",
       "         0.08461065,  0.05853987,  0.04510453,  0.11762285,  0.09296525,\n",
       "         0.04835368, -0.08911026,  0.00254138, -0.06312542, -0.02902064,\n",
       "         0.1027353 , -0.05375999, -0.00333334,  0.00395474,  0.00114667,\n",
       "         0.06687175,  0.01342786,  0.0918262 ,  0.09530977,  0.10069501,\n",
       "        -0.02727258,  0.09562106, -0.09734862,  0.05958501,  0.08793502,\n",
       "        -0.06369299,  0.11034398, -0.01416902,  0.05511556, -0.07880732,\n",
       "        -0.00718023,  0.07231972,  0.09363931, -0.07034803,  0.01521329,\n",
       "        -0.06248714, -0.03066569, -0.05979769,  0.08039437, -0.01081944,\n",
       "         0.07687622,  0.08989014,  0.05511495,  0.11203595,  0.08244383,\n",
       "        -0.06593874,  0.09835942, -0.06923767], dtype=float32),\n",
       " array([[[[-0.12032615],\n",
       "          [ 0.03522792],\n",
       "          [-0.00700566],\n",
       "          [ 0.01766693],\n",
       "          [ 0.04485671],\n",
       "          [-0.00910666],\n",
       "          [-0.0964934 ],\n",
       "          [-0.02739709],\n",
       "          [ 0.03016434],\n",
       "          [-0.01646961],\n",
       "          [-0.08258611],\n",
       "          [ 0.08638788],\n",
       "          [ 0.03954058],\n",
       "          [-0.06447516],\n",
       "          [ 0.02988189],\n",
       "          [-0.03547112],\n",
       "          [-0.02363245],\n",
       "          [-0.14216106],\n",
       "          [ 0.05627428],\n",
       "          [ 0.0736836 ],\n",
       "          [ 0.02655926],\n",
       "          [-0.00950262],\n",
       "          [-0.05799371],\n",
       "          [-0.01100146],\n",
       "          [-0.20659956],\n",
       "          [-0.12361198],\n",
       "          [-0.18553014],\n",
       "          [ 0.02765766],\n",
       "          [ 0.02153861],\n",
       "          [-0.08347307],\n",
       "          [ 0.04103724],\n",
       "          [ 0.00353666],\n",
       "          [-0.04899156],\n",
       "          [-0.06019253],\n",
       "          [ 0.01453058],\n",
       "          [ 0.06131657],\n",
       "          [-0.01226375],\n",
       "          [ 0.02662823],\n",
       "          [-0.15648353],\n",
       "          [-0.05906108],\n",
       "          [ 0.10440634],\n",
       "          [ 0.10039673],\n",
       "          [ 0.10946953],\n",
       "          [ 0.06365117],\n",
       "          [-0.0830145 ],\n",
       "          [ 0.01716467],\n",
       "          [ 0.03702043],\n",
       "          [-0.02825939],\n",
       "          [ 0.11161689],\n",
       "          [-0.03176963],\n",
       "          [ 0.04041643],\n",
       "          [ 0.06688755],\n",
       "          [ 0.081318  ],\n",
       "          [ 0.07038705],\n",
       "          [ 0.06603292],\n",
       "          [-0.00084783],\n",
       "          [-0.01446144],\n",
       "          [-0.06684049],\n",
       "          [ 0.02096971],\n",
       "          [-0.07015318],\n",
       "          [-0.09820787],\n",
       "          [ 0.03293191],\n",
       "          [ 0.04768808],\n",
       "          [ 0.0703628 ],\n",
       "          [-0.04478601],\n",
       "          [-0.0226077 ],\n",
       "          [-0.01891333],\n",
       "          [-0.07775104],\n",
       "          [-0.07812888],\n",
       "          [ 0.04513211],\n",
       "          [ 0.05231869],\n",
       "          [-0.02188228],\n",
       "          [ 0.01948031],\n",
       "          [-0.01201083],\n",
       "          [ 0.0331685 ],\n",
       "          [-0.07380598],\n",
       "          [-0.04653355],\n",
       "          [-0.18401252],\n",
       "          [-0.03510254],\n",
       "          [ 0.03752992],\n",
       "          [-0.02438627],\n",
       "          [-0.01505667],\n",
       "          [ 0.11856138],\n",
       "          [-0.02958313],\n",
       "          [ 0.10002147],\n",
       "          [-0.04125192],\n",
       "          [ 0.04330209],\n",
       "          [ 0.04180007],\n",
       "          [ 0.0261861 ],\n",
       "          [-0.0690875 ],\n",
       "          [-0.18703052],\n",
       "          [ 0.08877721],\n",
       "          [ 0.02503212],\n",
       "          [ 0.05204056],\n",
       "          [ 0.05723218],\n",
       "          [ 0.12766211],\n",
       "          [ 0.04238073],\n",
       "          [-0.20259081],\n",
       "          [-0.0597184 ],\n",
       "          [-0.2097403 ],\n",
       "          [-0.00799511],\n",
       "          [-0.0624061 ],\n",
       "          [ 0.09351061],\n",
       "          [ 0.05306654],\n",
       "          [ 0.02604705],\n",
       "          [ 0.02843495],\n",
       "          [-0.18292919],\n",
       "          [ 0.05444792],\n",
       "          [ 0.00666781],\n",
       "          [ 0.01172969],\n",
       "          [-0.04034841],\n",
       "          [-0.07702756],\n",
       "          [-0.11681432],\n",
       "          [ 0.02300507],\n",
       "          [-0.06125542],\n",
       "          [ 0.06978998],\n",
       "          [ 0.03540087],\n",
       "          [ 0.06834164],\n",
       "          [-0.02146214],\n",
       "          [-0.06149466],\n",
       "          [ 0.13283001],\n",
       "          [-0.13184291],\n",
       "          [-0.09310962],\n",
       "          [-0.20105487],\n",
       "          [ 0.03025258],\n",
       "          [ 0.04324125],\n",
       "          [-0.1687936 ],\n",
       "          [ 0.00159532]],\n",
       " \n",
       "         [[-0.12429173],\n",
       "          [ 0.06956642],\n",
       "          [-0.10377444],\n",
       "          [-0.02276478],\n",
       "          [ 0.05054696],\n",
       "          [ 0.04854604],\n",
       "          [ 0.02853345],\n",
       "          [ 0.09783146],\n",
       "          [-0.07056748],\n",
       "          [-0.03126165],\n",
       "          [-0.02521824],\n",
       "          [-0.09198238],\n",
       "          [-0.07304995],\n",
       "          [ 0.08153295],\n",
       "          [-0.0513765 ],\n",
       "          [ 0.03475857],\n",
       "          [-0.07930572],\n",
       "          [ 0.02479164],\n",
       "          [-0.02732658],\n",
       "          [-0.07838155],\n",
       "          [ 0.00664527],\n",
       "          [ 0.01897315],\n",
       "          [ 0.08082347],\n",
       "          [-0.06878402],\n",
       "          [-0.06845458],\n",
       "          [-0.05651817],\n",
       "          [-0.06148154],\n",
       "          [ 0.05883536],\n",
       "          [-0.04874663],\n",
       "          [ 0.05845813],\n",
       "          [ 0.03374501],\n",
       "          [-0.08011477],\n",
       "          [-0.02952665],\n",
       "          [-0.06761972],\n",
       "          [ 0.10298517],\n",
       "          [ 0.0306266 ],\n",
       "          [ 0.00708032],\n",
       "          [ 0.04740765],\n",
       "          [-0.00189681],\n",
       "          [ 0.09875842],\n",
       "          [ 0.04921447],\n",
       "          [-0.04184411],\n",
       "          [ 0.03980246],\n",
       "          [-0.02576746],\n",
       "          [-0.12398124],\n",
       "          [-0.02760504],\n",
       "          [ 0.06775883],\n",
       "          [ 0.01353467],\n",
       "          [-0.02200718],\n",
       "          [-0.03086299],\n",
       "          [-0.03425856],\n",
       "          [ 0.09883595],\n",
       "          [ 0.00454246],\n",
       "          [-0.02894358],\n",
       "          [-0.01552004],\n",
       "          [-0.01680122],\n",
       "          [-0.04681263],\n",
       "          [-0.07044566],\n",
       "          [ 0.04750242],\n",
       "          [ 0.09692921],\n",
       "          [ 0.10088519],\n",
       "          [-0.06335347],\n",
       "          [-0.01141045],\n",
       "          [-0.08316856],\n",
       "          [ 0.07865936],\n",
       "          [-0.08810232],\n",
       "          [-0.03083408],\n",
       "          [ 0.0230631 ],\n",
       "          [-0.00276374],\n",
       "          [-0.08237509],\n",
       "          [ 0.08112767],\n",
       "          [ 0.04254268],\n",
       "          [ 0.07718541],\n",
       "          [-0.01932038],\n",
       "          [ 0.0508804 ],\n",
       "          [-0.07415846],\n",
       "          [ 0.06264046],\n",
       "          [-0.02334158],\n",
       "          [-0.0586058 ],\n",
       "          [-0.08966723],\n",
       "          [-0.06790418],\n",
       "          [ 0.0060755 ],\n",
       "          [-0.08768795],\n",
       "          [-0.02140672],\n",
       "          [-0.08216139],\n",
       "          [ 0.05769404],\n",
       "          [-0.05000238],\n",
       "          [-0.05326594],\n",
       "          [-0.06827836],\n",
       "          [ 0.02182302],\n",
       "          [-0.02025399],\n",
       "          [-0.01731937],\n",
       "          [ 0.01790561],\n",
       "          [-0.05073211],\n",
       "          [ 0.0183872 ],\n",
       "          [-0.0816747 ],\n",
       "          [-0.10133231],\n",
       "          [ 0.02309486],\n",
       "          [-0.03459897],\n",
       "          [-0.13575822],\n",
       "          [ 0.07501261],\n",
       "          [-0.06446997],\n",
       "          [-0.05295645],\n",
       "          [ 0.06666542],\n",
       "          [ 0.04376178],\n",
       "          [ 0.03656238],\n",
       "          [-0.03989309],\n",
       "          [-0.04532918],\n",
       "          [-0.02340735],\n",
       "          [-0.00875074],\n",
       "          [-0.01021098],\n",
       "          [-0.13278091],\n",
       "          [-0.16065413],\n",
       "          [-0.04317419],\n",
       "          [ 0.04052513],\n",
       "          [-0.09145124],\n",
       "          [ 0.01009015],\n",
       "          [ 0.04484417],\n",
       "          [ 0.0889589 ],\n",
       "          [ 0.02655797],\n",
       "          [-0.01673301],\n",
       "          [ 0.03869734],\n",
       "          [ 0.03494057],\n",
       "          [-0.10679477],\n",
       "          [-0.0763474 ],\n",
       "          [-0.04934817],\n",
       "          [-0.04740371],\n",
       "          [ 0.10512892]]],\n",
       " \n",
       " \n",
       "        [[[-0.06237589],\n",
       "          [ 0.0126747 ],\n",
       "          [ 0.09725339],\n",
       "          [-0.09031072],\n",
       "          [-0.10433337],\n",
       "          [ 0.01527628],\n",
       "          [-0.02593141],\n",
       "          [ 0.03706821],\n",
       "          [ 0.04449091],\n",
       "          [ 0.10490488],\n",
       "          [ 0.0576196 ],\n",
       "          [ 0.07877906],\n",
       "          [-0.04630626],\n",
       "          [ 0.0419021 ],\n",
       "          [ 0.10350243],\n",
       "          [-0.00231486],\n",
       "          [-0.0548393 ],\n",
       "          [ 0.0613663 ],\n",
       "          [-0.07706756],\n",
       "          [ 0.00790829],\n",
       "          [ 0.0103775 ],\n",
       "          [-0.108962  ],\n",
       "          [-0.01373273],\n",
       "          [-0.07186689],\n",
       "          [-0.09297331],\n",
       "          [ 0.10402384],\n",
       "          [-0.16018419],\n",
       "          [-0.0554469 ],\n",
       "          [-0.03928697],\n",
       "          [ 0.08410744],\n",
       "          [-0.13793106],\n",
       "          [ 0.07502456],\n",
       "          [ 0.05913629],\n",
       "          [ 0.09785318],\n",
       "          [-0.05990203],\n",
       "          [ 0.00675314],\n",
       "          [ 0.09030896],\n",
       "          [-0.05769811],\n",
       "          [-0.12414874],\n",
       "          [-0.05290675],\n",
       "          [-0.04479546],\n",
       "          [-0.00815352],\n",
       "          [ 0.00817978],\n",
       "          [-0.09735625],\n",
       "          [-0.07328488],\n",
       "          [-0.06579012],\n",
       "          [-0.03622545],\n",
       "          [ 0.07161793],\n",
       "          [-0.04872208],\n",
       "          [ 0.08080484],\n",
       "          [-0.00122033],\n",
       "          [-0.06683335],\n",
       "          [ 0.06812464],\n",
       "          [-0.01206212],\n",
       "          [ 0.00932683],\n",
       "          [ 0.01729571],\n",
       "          [-0.00163384],\n",
       "          [ 0.11483731],\n",
       "          [-0.053315  ],\n",
       "          [ 0.05420735],\n",
       "          [-0.04342448],\n",
       "          [-0.06839068],\n",
       "          [-0.07890788],\n",
       "          [-0.00457227],\n",
       "          [ 0.02433361],\n",
       "          [ 0.03026857],\n",
       "          [ 0.0973056 ],\n",
       "          [ 0.11362585],\n",
       "          [-0.00021798],\n",
       "          [ 0.03701784],\n",
       "          [ 0.02826117],\n",
       "          [-0.1264165 ],\n",
       "          [-0.03709104],\n",
       "          [-0.180556  ],\n",
       "          [-0.04902034],\n",
       "          [-0.03731473],\n",
       "          [-0.04474222],\n",
       "          [-0.04342935],\n",
       "          [ 0.00053647],\n",
       "          [ 0.03771217],\n",
       "          [ 0.08932015],\n",
       "          [-0.13577728],\n",
       "          [-0.06450769],\n",
       "          [ 0.0127529 ],\n",
       "          [-0.0306095 ],\n",
       "          [ 0.00188244],\n",
       "          [ 0.06164316],\n",
       "          [ 0.04838262],\n",
       "          [-0.0314141 ],\n",
       "          [ 0.06916931],\n",
       "          [-0.16112791],\n",
       "          [-0.05544351],\n",
       "          [-0.13769513],\n",
       "          [ 0.01495417],\n",
       "          [-0.05938678],\n",
       "          [-0.04913267],\n",
       "          [ 0.03257683],\n",
       "          [-0.1917159 ],\n",
       "          [-0.03265715],\n",
       "          [-0.0544953 ],\n",
       "          [-0.06274996],\n",
       "          [-0.06969499],\n",
       "          [ 0.02551008],\n",
       "          [ 0.00543685],\n",
       "          [-0.00379085],\n",
       "          [-0.09680237],\n",
       "          [-0.06232248],\n",
       "          [-0.00916213],\n",
       "          [ 0.10119196],\n",
       "          [ 0.07216427],\n",
       "          [ 0.04903054],\n",
       "          [-0.02411225],\n",
       "          [-0.05961877],\n",
       "          [ 0.05086713],\n",
       "          [-0.02572914],\n",
       "          [ 0.02833749],\n",
       "          [-0.07752355],\n",
       "          [-0.05690549],\n",
       "          [ 0.03319396],\n",
       "          [ 0.14485495],\n",
       "          [-0.06946297],\n",
       "          [-0.07910531],\n",
       "          [ 0.05092282],\n",
       "          [ 0.00781315],\n",
       "          [-0.02465987],\n",
       "          [ 0.10547068],\n",
       "          [-0.16728555],\n",
       "          [-0.00842465]],\n",
       " \n",
       "         [[-0.1343776 ],\n",
       "          [-0.0824977 ],\n",
       "          [ 0.01561323],\n",
       "          [-0.01438729],\n",
       "          [ 0.05501003],\n",
       "          [-0.07013011],\n",
       "          [ 0.1300678 ],\n",
       "          [-0.06841582],\n",
       "          [-0.01114924],\n",
       "          [ 0.00162803],\n",
       "          [-0.11371351],\n",
       "          [ 0.06375477],\n",
       "          [ 0.01642966],\n",
       "          [ 0.05778229],\n",
       "          [ 0.0480459 ],\n",
       "          [ 0.12831418],\n",
       "          [-0.04923688],\n",
       "          [ 0.01062891],\n",
       "          [-0.10732047],\n",
       "          [ 0.07675331],\n",
       "          [-0.02943873],\n",
       "          [ 0.02344914],\n",
       "          [ 0.0355825 ],\n",
       "          [ 0.01991783],\n",
       "          [-0.0533872 ],\n",
       "          [ 0.12426629],\n",
       "          [-0.11350989],\n",
       "          [ 0.08423276],\n",
       "          [-0.05953879],\n",
       "          [ 0.02404568],\n",
       "          [ 0.06446006],\n",
       "          [-0.02417638],\n",
       "          [ 0.07848296],\n",
       "          [ 0.04554166],\n",
       "          [-0.06238612],\n",
       "          [-0.00070269],\n",
       "          [-0.09520061],\n",
       "          [ 0.01309258],\n",
       "          [-0.15659525],\n",
       "          [ 0.01907911],\n",
       "          [-0.08366164],\n",
       "          [ 0.09830723],\n",
       "          [-0.03011319],\n",
       "          [ 0.00129004],\n",
       "          [-0.14957944],\n",
       "          [-0.07414149],\n",
       "          [-0.02482112],\n",
       "          [-0.02366353],\n",
       "          [-0.08582816],\n",
       "          [ 0.01197137],\n",
       "          [ 0.06792247],\n",
       "          [-0.1043361 ],\n",
       "          [-0.07522067],\n",
       "          [-0.04048174],\n",
       "          [-0.05662474],\n",
       "          [ 0.09832411],\n",
       "          [ 0.07234729],\n",
       "          [-0.08007937],\n",
       "          [-0.02203589],\n",
       "          [-0.08547484],\n",
       "          [ 0.03754161],\n",
       "          [ 0.09211722],\n",
       "          [ 0.04837307],\n",
       "          [-0.01557648],\n",
       "          [ 0.00712995],\n",
       "          [ 0.02879433],\n",
       "          [-0.0541996 ],\n",
       "          [-0.0428887 ],\n",
       "          [ 0.02616904],\n",
       "          [-0.05523409],\n",
       "          [-0.09712497],\n",
       "          [-0.06049065],\n",
       "          [-0.01347825],\n",
       "          [ 0.03285732],\n",
       "          [ 0.07919353],\n",
       "          [-0.02914847],\n",
       "          [ 0.07484633],\n",
       "          [-0.01369669],\n",
       "          [ 0.05442128],\n",
       "          [ 0.02571398],\n",
       "          [ 0.04198746],\n",
       "          [-0.01687987],\n",
       "          [ 0.11167416],\n",
       "          [ 0.05716196],\n",
       "          [-0.03666969],\n",
       "          [-0.13280544],\n",
       "          [ 0.03203011],\n",
       "          [-0.05224352],\n",
       "          [ 0.03698905],\n",
       "          [ 0.08536752],\n",
       "          [ 0.06961469],\n",
       "          [ 0.03314687],\n",
       "          [ 0.07977416],\n",
       "          [ 0.01537336],\n",
       "          [ 0.08356642],\n",
       "          [ 0.07894124],\n",
       "          [ 0.06963762],\n",
       "          [-0.09090205],\n",
       "          [-0.00827761],\n",
       "          [ 0.04035932],\n",
       "          [ 0.04414472],\n",
       "          [ 0.03757969],\n",
       "          [ 0.036428  ],\n",
       "          [-0.05130699],\n",
       "          [-0.07597309],\n",
       "          [ 0.02502295],\n",
       "          [ 0.07170314],\n",
       "          [ 0.09361673],\n",
       "          [-0.01429005],\n",
       "          [ 0.00797288],\n",
       "          [ 0.08251169],\n",
       "          [-0.0081881 ],\n",
       "          [-0.17180595],\n",
       "          [ 0.04811973],\n",
       "          [ 0.10009037],\n",
       "          [ 0.0935823 ],\n",
       "          [-0.14311387],\n",
       "          [ 0.02547119],\n",
       "          [-0.07598882],\n",
       "          [ 0.02930511],\n",
       "          [-0.08485837],\n",
       "          [-0.00582402],\n",
       "          [-0.0040933 ],\n",
       "          [ 0.07837308],\n",
       "          [-0.0975071 ],\n",
       "          [ 0.04034321],\n",
       "          [ 0.02145671],\n",
       "          [ 0.05188359]]]], dtype=float32),\n",
       " array([-0.08726427], dtype=float32)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.layers[-1].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Container.get_weights of <keras.engine.training.Model object at 0x7fb0f8785a58>>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.get_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 14, 14, 32)   320         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 14, 14, 32)   0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 14, 14, 32)   0           leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 14, 14, 64)   18496       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 14, 14, 64)   0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 14, 14, 64)   0           leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 7, 7, 128)    73856       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 7, 7, 128)    0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 7, 7, 128)    0           leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 7, 7, 256)    295168      dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 7, 7, 256)    0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 7, 7, 256)    0           leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 12544)        0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "generation (Dense)              (None, 1)            12545       flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "auxiliary (Dense)               (None, 10)           125450      flatten_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 525,835\n",
      "Trainable params: 525,835\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "d.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_67 (Conv2D)           (None, 14, 14, 32)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_77 (LeakyReLU)   (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_68 (Conv2D)           (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_78 (LeakyReLU)   (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_69 (Conv2D)           (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_79 (LeakyReLU)   (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_70 (Conv2D)           (None, 7, 7, 256)         295168    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_80 (LeakyReLU)   (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_20 (Flatten)         (None, 12544)             0         \n",
      "=================================================================\n",
      "Total params: 387,840\n",
      "Trainable params: 387,840\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "d.layers[1].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[[-0.00466526, -0.00478109,  0.00469549, ..., -0.00466347,\n",
      "           0.00476041,  0.00474772],\n",
      "         [-0.00479064, -0.00486212,  0.00477491, ..., -0.00460266,\n",
      "           0.00475856,  0.00481216],\n",
      "         [ 0.00510463,  0.00503958, -0.00495022, ...,  0.0049541 ,\n",
      "          -0.00480145, -0.00484076],\n",
      "         ..., \n",
      "         [-0.00477714, -0.00485084,  0.00485832, ..., -0.00487119,\n",
      "           0.00488568,  0.0047249 ],\n",
      "         [-0.00456475, -0.0047423 ,  0.00471783, ..., -0.00464083,\n",
      "           0.0045321 ,  0.00459823],\n",
      "         [-0.00482344, -0.00469361,  0.00482364, ..., -0.00467242,\n",
      "           0.00495965,  0.00493532]],\n",
      "\n",
      "        [[-0.0048248 , -0.0049022 ,  0.00489049, ..., -0.00503888,\n",
      "           0.00488533,  0.00495924],\n",
      "         [-0.00493808, -0.00501448,  0.00489126, ..., -0.0047537 ,\n",
      "           0.00479108,  0.00483907],\n",
      "         [ 0.0045967 ,  0.00498764, -0.00463155, ...,  0.00486901,\n",
      "          -0.00486386, -0.00489598],\n",
      "         ..., \n",
      "         [-0.00472495, -0.00477746,  0.0046706 , ..., -0.0047403 ,\n",
      "           0.00465848,  0.0047026 ],\n",
      "         [-0.00464108, -0.0047769 ,  0.00483064, ..., -0.00485515,\n",
      "           0.00487282,  0.00480553],\n",
      "         [-0.00486707, -0.00502124,  0.00486421, ..., -0.0049884 ,\n",
      "           0.00515647,  0.00490911]],\n",
      "\n",
      "        [[-0.00485656, -0.00462757,  0.00476162, ..., -0.00468001,\n",
      "           0.00440295,  0.00476038],\n",
      "         [-0.00474873, -0.00474438,  0.0047616 , ..., -0.00477683,\n",
      "           0.00469349,  0.00469027],\n",
      "         [ 0.00493838,  0.00490787, -0.00483554, ...,  0.00491564,\n",
      "          -0.00493482, -0.00494918],\n",
      "         ..., \n",
      "         [-0.00485776, -0.00494081,  0.00467059, ..., -0.00487025,\n",
      "           0.00478642,  0.0049388 ],\n",
      "         [-0.00445642, -0.00449801,  0.00464002, ..., -0.00465505,\n",
      "           0.0045948 ,  0.00462566],\n",
      "         [-0.00484276, -0.00482042,  0.00485972, ..., -0.00480481,\n",
      "           0.00483678,  0.0046895 ]]],\n",
      "\n",
      "\n",
      "       [[[-0.00511662, -0.00513914,  0.00504583, ..., -0.00516876,\n",
      "           0.00510399,  0.00519526],\n",
      "         [-0.0049324 , -0.00494297,  0.00483113, ..., -0.00504538,\n",
      "           0.0049547 ,  0.00481963],\n",
      "         [ 0.00462014,  0.00471146, -0.00443826, ...,  0.00463149,\n",
      "          -0.00479665, -0.00467067],\n",
      "         ..., \n",
      "         [-0.00475914, -0.00485047,  0.00483677, ..., -0.00481647,\n",
      "           0.00476353,  0.00506634],\n",
      "         [-0.00524257, -0.00511141,  0.00501607, ..., -0.00505686,\n",
      "           0.00522965,  0.00517876],\n",
      "         [-0.00485145, -0.00490876,  0.00463881, ..., -0.00483739,\n",
      "           0.0047763 ,  0.00475398]],\n",
      "\n",
      "        [[-0.00460659, -0.00458494,  0.00467362, ..., -0.00458547,\n",
      "           0.00456655,  0.00455698],\n",
      "         [-0.00478622, -0.0048841 ,  0.00477733, ..., -0.0051061 ,\n",
      "           0.00493518,  0.00490938],\n",
      "         [ 0.004936  ,  0.0046329 , -0.00505381, ...,  0.00490016,\n",
      "          -0.00488029, -0.00476845],\n",
      "         ..., \n",
      "         [-0.00510599, -0.00515261,  0.00517649, ..., -0.00509371,\n",
      "           0.00523731,  0.00525881],\n",
      "         [-0.00496678, -0.00494319,  0.00492002, ..., -0.00485589,\n",
      "           0.00488469,  0.00494243],\n",
      "         [-0.0046756 , -0.00453077,  0.00485264, ..., -0.00482732,\n",
      "           0.00479682,  0.00481379]],\n",
      "\n",
      "        [[-0.00508557, -0.00515948,  0.00498597, ..., -0.00513181,\n",
      "           0.00526657,  0.00536018],\n",
      "         [-0.004894  , -0.00494631,  0.00494309, ..., -0.00516909,\n",
      "           0.00499772,  0.00495978],\n",
      "         [ 0.00461728,  0.00477717, -0.00462413, ...,  0.00485639,\n",
      "          -0.00471339, -0.00466356],\n",
      "         ..., \n",
      "         [-0.00478803, -0.00495135,  0.00481721, ..., -0.0049703 ,\n",
      "           0.00492838,  0.00482356],\n",
      "         [-0.0050877 , -0.00519534,  0.00510011, ..., -0.00509039,\n",
      "           0.00521319,  0.00519542],\n",
      "         [-0.00481146, -0.00474242,  0.00464063, ..., -0.00472121,\n",
      "           0.00467558,  0.00482942]]],\n",
      "\n",
      "\n",
      "       [[[-0.0045031 , -0.00444888,  0.00459561, ..., -0.00468547,\n",
      "           0.00461574,  0.00453013],\n",
      "         [-0.00441813, -0.00450622,  0.00463298, ..., -0.00472924,\n",
      "           0.00466849,  0.00473589],\n",
      "         [ 0.00446033,  0.00452684, -0.00489022, ...,  0.00486424,\n",
      "          -0.00482671, -0.00488106],\n",
      "         ..., \n",
      "         [-0.00459143, -0.00454606,  0.00449805, ..., -0.0046371 ,\n",
      "           0.00466478,  0.0045694 ],\n",
      "         [-0.00436753, -0.00445775,  0.00456088, ..., -0.00472751,\n",
      "           0.0046138 ,  0.00460182],\n",
      "         [-0.00443648, -0.00468412,  0.00476526, ..., -0.00488581,\n",
      "           0.00464141,  0.00468816]],\n",
      "\n",
      "        [[-0.00470971, -0.00471342,  0.00467033, ..., -0.00475047,\n",
      "           0.00487564,  0.0048258 ],\n",
      "         [-0.00445606, -0.00427251,  0.00462027, ..., -0.00454136,\n",
      "           0.00467219,  0.00465597],\n",
      "         [ 0.00457476,  0.00443162, -0.00448123, ...,  0.0045182 ,\n",
      "          -0.00455106, -0.00465703],\n",
      "         ..., \n",
      "         [-0.00429616, -0.00434851,  0.00435744, ..., -0.00455152,\n",
      "           0.00444656,  0.00440546],\n",
      "         [-0.00447266, -0.0045507 ,  0.0044522 , ..., -0.00468408,\n",
      "           0.00462283,  0.00461541],\n",
      "         [-0.00469366, -0.00469324,  0.00457525, ..., -0.00466438,\n",
      "           0.00447829,  0.00469715]],\n",
      "\n",
      "        [[-0.00434622, -0.00463533,  0.00461063, ..., -0.00470267,\n",
      "           0.00483489,  0.00435288],\n",
      "         [-0.00455833, -0.00465869,  0.00460966, ..., -0.00450056,\n",
      "           0.00474363,  0.00473361],\n",
      "         [ 0.00469122,  0.00463637, -0.00477655, ...,  0.00468467,\n",
      "          -0.00479444, -0.0047916 ],\n",
      "         ..., \n",
      "         [-0.00448951, -0.00441176,  0.00474331, ..., -0.00452087,\n",
      "           0.00462047,  0.00456091],\n",
      "         [-0.00468461, -0.00461524,  0.0045663 , ..., -0.00469671,\n",
      "           0.00462966,  0.00462467],\n",
      "         [-0.00455344, -0.00479191,  0.00478919, ..., -0.0049227 ,\n",
      "           0.00491377,  0.00489004]]]], dtype=float32), array([-0.23989208, -0.25275579,  0.27744958, -0.28658861,  0.28476501,\n",
      "       -0.2906161 ,  0.25421157, -0.26440755,  0.25751036, -0.19854026,\n",
      "        0.27531242,  0.25305623,  0.27264068,  0.2544603 , -0.31208614,\n",
      "        0.2616443 , -0.26512209,  0.26780611,  0.27062565,  0.26458576,\n",
      "        0.26425186, -0.28836596,  0.26369846,  0.26514947,  0.27361178,\n",
      "        0.25642875,  0.27455568, -0.28932938, -0.25296894,  0.26115525,\n",
      "        0.26246327, -0.24162824,  0.26252139,  0.26828024,  0.24750961,\n",
      "        0.27150318, -0.26035583, -0.24557017, -0.28971201,  0.2703054 ,\n",
      "        0.26570702,  0.2569336 , -0.2638385 ,  0.2798492 , -0.23873323,\n",
      "        0.26828238,  0.26815414, -0.26264095, -0.24979769,  0.27290529,\n",
      "       -0.26914135,  0.27160141, -0.28300929,  0.26780424,  0.2629931 ,\n",
      "        0.28524929,  0.26754916,  0.27900577,  0.27796263,  0.276274  ,\n",
      "       -0.22887085, -0.26929682, -0.2780264 , -0.28067783,  0.26304403,\n",
      "        0.2661536 ,  0.25023136, -0.29193443,  0.2672416 ,  0.26017562,\n",
      "       -0.28184763,  0.26896945, -0.27499121,  0.28118211, -0.28478321,\n",
      "        0.27428898,  0.27434224,  0.28214234,  0.26708752, -0.26734447,\n",
      "        0.26199034,  0.26900727,  0.26509649,  0.27344283, -0.26742348,\n",
      "       -0.26222086,  0.25667125,  0.26243171, -0.21232668,  0.25923234,\n",
      "       -0.25312597,  0.26867583, -0.26497322,  0.26861107,  0.26537269,\n",
      "        0.25845274, -0.27264857,  0.25753528,  0.26943398,  0.27936712,\n",
      "       -0.22591904, -0.27149928, -0.25339791,  0.27220911,  0.27872914,\n",
      "        0.27149206,  0.26738212,  0.2760078 ,  0.27880874,  0.26582468,\n",
      "        0.26658723, -0.21978994, -0.23521221,  0.26475367,  0.26579234,\n",
      "        0.26499286, -0.26725784,  0.26396081, -0.24737425,  0.26912773,\n",
      "       -0.30224308,  0.26858491,  0.27248487, -0.26584151, -0.26580232,\n",
      "       -0.27690464,  0.26575038,  0.28245825], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "w = d.layers[1].layers[6].get_weights()\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3, 1, 32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.convolutional.Conv2D at 0x7fb0f8a83b70>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.layers[1].layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate real, fake batches"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
